{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import torch\n",
    "import mcubes\n",
    "import numpy as np\n",
    "from skgstat import Variogram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "pth_file = 'experiments/suzanne/set100/models/M0.pth'\n",
    "nerf_model = torch.load(pth_file).to(device)\n",
    "\n",
    "# Generate the grid of points for XYZ\n",
    "N = 30\n",
    "scale = 1.5\n",
    "x = torch.linspace(-scale, scale, N, device=device)\n",
    "y = torch.linspace(-scale, scale, N, device=device)\n",
    "z = torch.linspace(-scale, scale, N, device=device)\n",
    "x, y, z = torch.meshgrid((x, y, z))\n",
    "xyz = torch.cat((x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)), dim=1)  # [125000, 3] points\n",
    "xyzDirection = torch.zeros_like(xyz).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate densities\n",
    "_, density = nerf_model.forward(xyz, xyzDirection)  # [125000]\n",
    "density = density.cpu().detach().numpy()  # Convert to numpy array\n",
    "xyz = xyz.cpu().detach().numpy()  # Convert to numpy array\n",
    "\n",
    "# Apply masking to reduce the number of points\n",
    "threshold_value = np.quantile(density, 0.5)  # Use median density as threshold\n",
    "mask = density >= threshold_value  # Mask for high-density points\n",
    "\n",
    "# Additional mask (e.g., only points with positive z-values)\n",
    "z_positive_mask = xyz[:, 2] >= 0\n",
    "combined_mask = mask & z_positive_mask\n",
    "\n",
    "# Filter points and density using the mask\n",
    "filtered_xyz = xyz[combined_mask]\n",
    "filtered_density = density[combined_mask]\n",
    "\n",
    "filtered_xyz_scaled = (filtered_xyz / N) * (2 * scale) - scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the variogram using the filtered and scaled points\n",
    "variogram = Variogram(\n",
    "    filtered_xyz, \n",
    "    filtered_density, \n",
    "    model='spherical', \n",
    "    normalize=False, \n",
    "    nugget=0.1\n",
    ")\n",
    "\n",
    "# Describe the fitted variogram\n",
    "print(variogram.describe())\n",
    "\n",
    "# Plot the variogram\n",
    "variogram.plot()\n",
    "plt.title(\"3D Variogram for Filtered NeRF Densities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "pairwise_distances = squareform(pdist(filtered_xyz))  # [5080, 5080]\n",
    "\n",
    "# Get the bin edges and experimental variogram values\n",
    "bin_edges = variogram.bins  # Bin edges (distance ranges)\n",
    "bin_uncertainties = variogram.experimental  # Semi-variance per bin\n",
    "\n",
    "# Initialize uncertainties for each point\n",
    "point_uncertainties = np.zeros(filtered_xyz.shape[0])\n",
    "\n",
    "# Assign uncertainties based on the average bin of the point's neighbors\n",
    "for i in range(filtered_xyz.shape[0]):\n",
    "    # Find distances from the current point\n",
    "    distances = pairwise_distances[i]\n",
    "    \n",
    "    # Find which bin each distance falls into\n",
    "    bin_indices = np.digitize(distances, bin_edges, right=True)\n",
    "    \n",
    "    # Assign the uncertainty as the average uncertainty of its neighbors' bins\n",
    "    neighbor_uncertainties = bin_uncertainties[bin_indices - 1]  # Adjust index\n",
    "    point_uncertainties[i] = np.mean(neighbor_uncertainties)\n",
    "\n",
    "# Normalize point uncertainties\n",
    "point_uncertainties = (point_uncertainties - np.min(point_uncertainties)) / (\n",
    "    np.max(point_uncertainties) - np.min(point_uncertainties)\n",
    ")\n",
    "\n",
    "# assert len(uncertainty) == len(filtered_xyz) \n",
    "# print(len(uncertainty)); print(len(filtered_xyz))\n",
    "# Assign colors based on uncertainty using a colormap\n",
    "colormap = cm.get_cmap('inferno')\n",
    "colors = colormap(point_uncertainties)[:, :3]  # Get RGB values\n",
    "\n",
    "# Create a PointCloud with colors\n",
    "point_cloud = trimesh.points.PointCloud(filtered_xyz, colors=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the uncertainty threshold (e.g., 0.5)\n",
    "threshold = 0.5\n",
    "\n",
    "# Filter points, uncertainties, and colors based on the threshold\n",
    "threshold_mask = point_uncertainties >= threshold  # Keep points with uncertainty >= threshold\n",
    "filtered_xyz_thresholded = filtered_xyz[threshold_mask]\n",
    "point_uncertainties_thresholded = point_uncertainties[threshold_mask]\n",
    "colors_thresholded = colors[threshold_mask]\n",
    "\n",
    "# Define sphere sizes for the remaining points\n",
    "sphere_sizes_thresholded = 0.05 + point_uncertainties_thresholded * 0.01\n",
    "# Create spheres for the thresholded points\n",
    "spheres = []\n",
    "for point, size, color in zip(filtered_xyz_thresholded, sphere_sizes_thresholded, colors_thresholded):\n",
    "    sphere = trimesh.primitives.Sphere(\n",
    "        radius=size, center=point, subdivisions=2  # Subdivisions for smoothness\n",
    "    )\n",
    "    # Apply the color to the sphere\n",
    "    sphere.visual.vertex_colors = (color * 255).astype(np.uint8)\n",
    "    spheres.append(sphere)\n",
    "\n",
    "# Add the spheres and the mesh to the scene\n",
    "density_np = density.reshape(N, N, N)\n",
    "vertices, triangles = mcubes.marching_cubes(density_np, 5 * np.mean(density_np))\n",
    "vertices_scaled = (vertices / N) * (2 * scale) - scale\n",
    "mesh = trimesh.Trimesh(vertices_scaled, triangles)\n",
    "scene = trimesh.Scene([mesh] + spheres)\n",
    "\n",
    "# Show the scene\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density_np = density.reshape(N, N, N)\n",
    "# vertices, triangles = mcubes.marching_cubes(density_np, 5 * np.mean(density_np))\n",
    "# vertices_scaled = (vertices / N) * (2 * scale) - scale\n",
    "# mesh = trimesh.Trimesh(vertices_scaled, triangles)\n",
    "# scene = trimesh.Scene([mesh, point_cloud])\n",
    "# scene.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting Color-based Variogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import torch\n",
    "import mcubes\n",
    "import numpy as np\n",
    "from skgstat import Variogram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "device = 'cuda'\n",
    "pth_file = 'experiments/suzanne/set100/models/M0.pth'\n",
    "# pth_file = \"experiments/test/set30/models/M0.pth\"\n",
    "nerf_model = torch.load(pth_file).to(device)\n",
    "\n",
    "# Generate the grid of points for XYZ\n",
    "N = 35\n",
    "scale = 1.5\n",
    "x = torch.linspace(-scale, scale, N, device=device)\n",
    "y = torch.linspace(-scale, scale, N, device=device)\n",
    "z = torch.linspace(-scale, scale, N, device=device)\n",
    "x, y, z = torch.meshgrid((x, y, z))\n",
    "xyz = torch.cat((x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)), dim=1)  # [125000, 3] points\n",
    "\n",
    "# Define a view direction for color queries (e.g., along the z-axis)\n",
    "view_direction = torch.tensor([0, 0, 1], device=device).expand(xyz.shape[0], -1)\n",
    "\n",
    "# Query the NeRF model for colors (assumes `forward` outputs RGB and density)\n",
    "rgb, _ = nerf_model.forward(xyz, view_direction)  # [125000, 3]\n",
    "\n",
    "# Convert to numpy for processing\n",
    "xyz = xyz.cpu().detach().numpy()\n",
    "rgb = rgb.cpu().detach().numpy()\n",
    "\n",
    "# Apply masking to reduce the number of points\n",
    "threshold_value = np.quantile(rgb.mean(axis=1), 0.5)  # Use median RGB intensity as threshold\n",
    "mask = rgb.mean(axis=1) >= threshold_value  # Mask based only on RGB intensity\n",
    "\n",
    "# Filter points and colors using the mask\n",
    "filtered_xyz = xyz[mask]\n",
    "filtered_rgb = rgb[mask]\n",
    "\n",
    "# Compute the variogram for color\n",
    "color_variogram = Variogram(\n",
    "    filtered_xyz, \n",
    "    filtered_rgb.mean(axis=1),  # Use mean intensity as a scalar representation of color\n",
    "    model='spherical', \n",
    "    normalize=False, \n",
    "    nugget=0.1\n",
    ")\n",
    "\n",
    "# Describe the fitted variogram\n",
    "print(color_variogram.describe())\n",
    "\n",
    "# Plot the variogram\n",
    "color_variogram.plot()\n",
    "plt.title(\"3D Variogram for Filtered NeRF Colors\")\n",
    "plt.show()\n",
    "\n",
    "# Compute pairwise distances between points\n",
    "pairwise_distances = squareform(pdist(filtered_xyz))  # [N, N]\n",
    "\n",
    "# Get the bin edges and experimental variogram values\n",
    "bin_edges = color_variogram.bins  # Bin edges (distance ranges)\n",
    "bin_uncertainties = color_variogram.experimental  # Semi-variance per bin\n",
    "\n",
    "# Initialize uncertainties for each point\n",
    "point_uncertainties = np.zeros(filtered_xyz.shape[0])\n",
    "\n",
    "# Assign uncertainties based on the average bin of the point's neighbors\n",
    "for i in range(filtered_xyz.shape[0]):\n",
    "    # Find distances from the current point\n",
    "    distances = pairwise_distances[i]\n",
    "    \n",
    "    # Find which bin each distance falls into\n",
    "    bin_indices = np.digitize(distances, bin_edges, right=True)\n",
    "    \n",
    "    # Assign the uncertainty as the average uncertainty of its neighbors' bins\n",
    "    neighbor_uncertainties = bin_uncertainties[bin_indices - 1]  # Adjust index\n",
    "    point_uncertainties[i] = np.mean(neighbor_uncertainties)\n",
    "\n",
    "# Normalize point uncertainties\n",
    "point_uncertainties = (point_uncertainties - np.min(point_uncertainties)) / (\n",
    "    np.max(point_uncertainties) - np.min(point_uncertainties)\n",
    ")\n",
    "\n",
    "# Assign colors based on uncertainty using a colormap\n",
    "colormap = cm.get_cmap('inferno')\n",
    "colors = colormap(point_uncertainties)[:, :3]  # Get RGB values\n",
    "\n",
    "# Create a PointCloud with colors\n",
    "point_cloud = trimesh.points.PointCloud(filtered_xyz, colors=colors)\n",
    "\n",
    "# Define the uncertainty threshold (e.g., 0.5)\n",
    "threshold = 0.99\n",
    "\n",
    "# Filter points, uncertainties, and colors based on the threshold\n",
    "threshold_mask = point_uncertainties >= threshold  # Keep points with uncertainty >= threshold\n",
    "filtered_xyz_thresholded = filtered_xyz[threshold_mask]\n",
    "point_uncertainties_thresholded = point_uncertainties[threshold_mask]\n",
    "colors_thresholded = colors[threshold_mask]\n",
    "\n",
    "# Define sphere sizes for the remaining points\n",
    "sphere_sizes_thresholded = 0.05 + point_uncertainties_thresholded * 0.001\n",
    "# Create spheres for the thresholded points\n",
    "spheres = []\n",
    "for point, size, color in zip(filtered_xyz_thresholded, sphere_sizes_thresholded, colors_thresholded):\n",
    "    sphere = trimesh.primitives.Sphere(\n",
    "        radius=size, center=point, subdivisions=2  # Subdivisions for smoothness\n",
    "    )\n",
    "    # Apply the color to the sphere\n",
    "    sphere.visual.vertex_colors = (color * 255).astype(np.uint8)\n",
    "    spheres.append(sphere)\n",
    "\n",
    "# Add the spheres and the mesh to the scene\n",
    "density_np = rgb.mean(axis=1).reshape(N, N, N)  # Example for visualization\n",
    "vertices, triangles = mcubes.marching_cubes(density_np, 3 * np.mean(density_np))\n",
    "vertices_scaled = (vertices / N) * (2 * scale) - scale\n",
    "mesh = trimesh.Trimesh(vertices_scaled, triangles)\n",
    "scene = trimesh.Scene([mesh] + spheres)\n",
    "\n",
    "# Show the scene\n",
    "scene.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens if we filter based on the density gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import torch\n",
    "import mcubes\n",
    "import numpy as np\n",
    "from skgstat import Variogram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "device = 'cuda'\n",
    "pth_file = 'experiments/suzanne/set100/models/M0.pth'\n",
    "nerf_model = torch.load(pth_file).to(device)\n",
    "\n",
    "# Generate the grid of points for XYZ\n",
    "N = 35\n",
    "scale = 1.5\n",
    "x = torch.linspace(-scale, scale, N, device=device)\n",
    "y = torch.linspace(-scale, scale, N, device=device)\n",
    "z = torch.linspace(-scale, scale, N, device=device)\n",
    "x, y, z = torch.meshgrid((x, y, z))\n",
    "xyz = torch.cat((x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)), dim=1).to(device)  # [N^3, 3]\n",
    "xyz.requires_grad_(True)  # Ensure gradients for density gradient computation\n",
    "\n",
    "# Define a view direction for color queries (e.g., along the z-axis)\n",
    "view_direction = torch.tensor([0, 0, 1], device=device).expand(xyz.shape[0], -1)\n",
    "\n",
    "# Query the NeRF model for colors and density\n",
    "rgb, density = nerf_model.forward(xyz, view_direction)  # RGB: [N^3, 3], Density: [N^3]\n",
    "\n",
    "# Compute the density gradient\n",
    "density_gradient = torch.autograd.grad(\n",
    "    outputs=density,\n",
    "    inputs=xyz,\n",
    "    grad_outputs=torch.ones_like(density),  # Same shape as density\n",
    "    create_graph=True,\n",
    ")[0]  # [N^3, 3]\n",
    "\n",
    "# Calculate gradient magnitude\n",
    "gradient_magnitude = torch.norm(density_gradient, dim=-1)  # [N^3]\n",
    "\n",
    "# Apply the gradient magnitude mask\n",
    "threshold = 3  # Define your threshold value\n",
    "gradient_mask = gradient_magnitude >= threshold\n",
    "\n",
    "# Filter points and attributes based on the mask\n",
    "filtered_xyz = xyz[gradient_mask].cpu().detach().numpy()\n",
    "filtered_rgb = rgb[gradient_mask].cpu().detach().numpy()\n",
    "filtered_density = density[gradient_mask].cpu().detach().numpy()\n",
    "\n",
    "# Compute the variogram for color\n",
    "color_variogram = Variogram(\n",
    "    filtered_xyz, \n",
    "    filtered_rgb.mean(axis=1),  # Use mean intensity as a scalar representation of color\n",
    "    model='spherical', \n",
    "    normalize=False, \n",
    "    nugget=0.1\n",
    ")\n",
    "\n",
    "# Describe the fitted variogram\n",
    "print(color_variogram.describe())\n",
    "\n",
    "# Plot the variogram\n",
    "color_variogram.plot()\n",
    "plt.title(\"3D Variogram for Filtered NeRF Colors\")\n",
    "plt.show()\n",
    "\n",
    "# Compute pairwise distances between points\n",
    "pairwise_distances = squareform(pdist(filtered_xyz))  # [N_filtered, N_filtered]\n",
    "\n",
    "# Get the bin edges and experimental variogram values\n",
    "bin_edges = color_variogram.bins  # Bin edges (distance ranges)\n",
    "bin_uncertainties = color_variogram.experimental  # Semi-variance per bin\n",
    "\n",
    "# Initialize uncertainties for each point\n",
    "point_uncertainties = np.zeros(filtered_xyz.shape[0])\n",
    "\n",
    "# Assign uncertainties based on the average bin of the point's neighbors\n",
    "for i in range(filtered_xyz.shape[0]):\n",
    "    # Find distances from the current point\n",
    "    distances = pairwise_distances[i]\n",
    "    \n",
    "    # Find which bin each distance falls into\n",
    "    bin_indices = np.digitize(distances, bin_edges, right=True)\n",
    "    \n",
    "    # Assign the uncertainty as the average uncertainty of its neighbors' bins\n",
    "    neighbor_uncertainties = bin_uncertainties[bin_indices - 1]  # Adjust index\n",
    "    point_uncertainties[i] = np.mean(neighbor_uncertainties)\n",
    "\n",
    "# Normalize point uncertainties\n",
    "point_uncertainties = (point_uncertainties - np.min(point_uncertainties)) / (\n",
    "    np.max(point_uncertainties) - np.min(point_uncertainties)\n",
    ")\n",
    "\n",
    "# Assign colors based on uncertainty using a colormap\n",
    "colormap = cm.get_cmap('inferno')\n",
    "colors = colormap(point_uncertainties)[:, :3]  # Get RGB values\n",
    "\n",
    "# Define an uncertainty threshold\n",
    "threshold_uncertainty = 0.9  # Adjust this value to filter spheres\n",
    "\n",
    "# Filter points, sizes, and colors based on the uncertainty threshold\n",
    "threshold_mask = point_uncertainties >= threshold_uncertainty\n",
    "filtered_xyz_thresholded = filtered_xyz[threshold_mask]\n",
    "point_uncertainties_thresholded = point_uncertainties[threshold_mask]\n",
    "colors_thresholded = colors[threshold_mask]\n",
    "sphere_sizes_thresholded = 0.05 + point_uncertainties_thresholded * 0.001  # Adjust size based on uncertainty\n",
    "\n",
    "# Create spheres for the filtered points\n",
    "spheres = []\n",
    "for point, size, color in zip(filtered_xyz_thresholded, sphere_sizes_thresholded, colors_thresholded):\n",
    "    sphere = trimesh.primitives.Sphere(\n",
    "        radius=size, center=point, subdivisions=2  # Subdivisions for smoothness\n",
    "    )\n",
    "    # Apply the color to the sphere\n",
    "    sphere.visual.vertex_colors = (color * 255).astype(np.uint8)\n",
    "    spheres.append(sphere)\n",
    "\n",
    "\n",
    "# Add the mesh for spatial context\n",
    "density_np = density.cpu().detach().numpy().reshape(N, N, N)  # Use the full density array\n",
    "vertices, triangles = mcubes.marching_cubes(density_np, 3 * np.mean(density_np))  # Adjust threshold as needed\n",
    "vertices_scaled = (vertices / N) * (2 * scale) - scale\n",
    "mesh = trimesh.Trimesh(vertices_scaled, triangles)\n",
    "\n",
    "# Combine the mesh and spheres into a single scene\n",
    "scene = trimesh.Scene([mesh] + spheres)\n",
    "\n",
    "# Show the combined scene\n",
    "scene.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just density gradient based didn't work well. What about filtering the points with sufficently high densities and gradients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import torch\n",
    "import mcubes\n",
    "import numpy as np\n",
    "from skgstat import Variogram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "device = 'cuda'\n",
    "pth_file = 'experiments/suzanne/set100/models/M0.pth'\n",
    "nerf_model = torch.load(pth_file).to(device)\n",
    "\n",
    "# Generate the grid of points for XYZ\n",
    "N = 35\n",
    "scale = 1.5\n",
    "x = torch.linspace(-scale, scale, N, device=device)\n",
    "y = torch.linspace(-scale, scale, N, device=device)\n",
    "z = torch.linspace(-scale, scale, N, device=device)\n",
    "x, y, z = torch.meshgrid((x, y, z))\n",
    "xyz = torch.cat((x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)), dim=1).to(device)  # [N^3, 3]\n",
    "xyz.requires_grad_(True)  # Ensure gradients for density gradient computation\n",
    "\n",
    "# Define a view direction for color queries (e.g., along the z-axis)\n",
    "view_direction = torch.tensor([0, 0, 1], device=device).expand(xyz.shape[0], -1)\n",
    "\n",
    "# Query the NeRF model for colors and density\n",
    "rgb, density = nerf_model.forward(xyz, view_direction)  # RGB: [N^3, 3], Density: [N^3]\n",
    "\n",
    "# Compute the density gradient\n",
    "density_gradient = torch.autograd.grad(\n",
    "    outputs=density,\n",
    "    inputs=xyz,\n",
    "    grad_outputs=torch.ones_like(density),  # Same shape as density\n",
    "    create_graph=True,\n",
    ")[0]  # [N^3, 3]\n",
    "\n",
    "# Calculate gradient magnitude\n",
    "gradient_magnitude = torch.norm(density_gradient, dim=-1)  # [N^3]\n",
    "\n",
    "# Apply a combined mask for density and gradient magnitude\n",
    "gradient_threshold = 2  # Threshold for gradient magnitude\n",
    "density_threshold = 0.5 * density.max()  # Threshold for density (e.g., 50% of max density)\n",
    "\n",
    "combined_mask = (gradient_magnitude >= gradient_threshold) & (density >= density_threshold)\n",
    "\n",
    "# Filter points and attributes based on the combined mask\n",
    "filtered_xyz = xyz[combined_mask].cpu().detach().numpy()\n",
    "filtered_rgb = rgb[combined_mask].cpu().detach().numpy()\n",
    "filtered_density = density[combined_mask].cpu().detach().numpy()\n",
    "\n",
    "# Compute the variogram for color\n",
    "color_variogram = Variogram(\n",
    "    filtered_xyz, \n",
    "    filtered_rgb.mean(axis=1),  # Use mean intensity as a scalar representation of color\n",
    "    model='spherical', \n",
    "    normalize=False, \n",
    "    nugget=0.1\n",
    ")\n",
    "\n",
    "# Describe the fitted variogram\n",
    "print(color_variogram.describe())\n",
    "\n",
    "# Plot the variogram\n",
    "color_variogram.plot()\n",
    "plt.title(\"3D Variogram for Filtered NeRF Colors\")\n",
    "plt.show()\n",
    "\n",
    "# Compute pairwise distances between points\n",
    "pairwise_distances = squareform(pdist(filtered_xyz))  # [N_filtered, N_filtered]\n",
    "\n",
    "# Get the bin edges and experimental variogram values\n",
    "bin_edges = color_variogram.bins  # Bin edges (distance ranges)\n",
    "bin_uncertainties = color_variogram.experimental  # Semi-variance per bin\n",
    "\n",
    "# Initialize uncertainties for each point\n",
    "point_uncertainties = np.zeros(filtered_xyz.shape[0])\n",
    "\n",
    "# Assign uncertainties based on the average bin of the point's neighbors\n",
    "for i in range(filtered_xyz.shape[0]):\n",
    "    # Find distances from the current point\n",
    "    distances = pairwise_distances[i]\n",
    "    \n",
    "    # Find which bin each distance falls into\n",
    "    bin_indices = np.digitize(distances, bin_edges, right=True)\n",
    "    \n",
    "    # Assign the uncertainty as the average uncertainty of its neighbors' bins\n",
    "    neighbor_uncertainties = bin_uncertainties[bin_indices - 1]  # Adjust index\n",
    "    point_uncertainties[i] = np.mean(neighbor_uncertainties)\n",
    "\n",
    "# Normalize point uncertainties\n",
    "point_uncertainties = (point_uncertainties - np.min(point_uncertainties)) / (\n",
    "    np.max(point_uncertainties) - np.min(point_uncertainties)\n",
    ")\n",
    "\n",
    "# Assign colors based on uncertainty using a colormap\n",
    "colormap = cm.get_cmap('inferno')\n",
    "colors = colormap(point_uncertainties)[:, :3]  # Get RGB values\n",
    "\n",
    "# Define an uncertainty threshold for spheres\n",
    "threshold_uncertainty = 0.8  # Adjust this value to filter spheres\n",
    "threshold_mask = point_uncertainties >= threshold_uncertainty\n",
    "filtered_xyz_thresholded = filtered_xyz[threshold_mask]\n",
    "point_uncertainties_thresholded = point_uncertainties[threshold_mask]\n",
    "colors_thresholded = colors[threshold_mask]\n",
    "sphere_sizes_thresholded = 0.05 + point_uncertainties_thresholded * 0.001  # Adjust size based on uncertainty\n",
    "\n",
    "# Create spheres for the filtered points\n",
    "spheres = []\n",
    "for point, size, color in zip(filtered_xyz_thresholded, sphere_sizes_thresholded, colors_thresholded):\n",
    "    sphere = trimesh.primitives.Sphere(\n",
    "        radius=size, center=point, subdivisions=2  # Subdivisions for smoothness\n",
    "    )\n",
    "    # Apply the color to the sphere\n",
    "    sphere.visual.vertex_colors = (color * 255).astype(np.uint8)\n",
    "    spheres.append(sphere)\n",
    "\n",
    "# Add the mesh for spatial context\n",
    "density_np = density.cpu().detach().numpy().reshape(N, N, N)  # Use the full density array\n",
    "vertices, triangles = mcubes.marching_cubes(density_np, 3 * np.mean(density_np))  # Adjust threshold as needed\n",
    "vertices_scaled = (vertices / N) * (2 * scale) - scale\n",
    "mesh = trimesh.Trimesh(vertices_scaled, triangles)\n",
    "\n",
    "# Combine the mesh and spheres into a single scene\n",
    "scene = trimesh.Scene([mesh] + spheres)\n",
    "\n",
    "# Show the combined scene\n",
    "scene.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New combination of masking: rgb and density gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import torch\n",
    "import mcubes\n",
    "import numpy as np\n",
    "from skgstat import Variogram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "device = 'cuda'\n",
    "pth_file = 'experiments/suzanne/set100/models/M0.pth'\n",
    "nerf_model = torch.load(pth_file).to(device)\n",
    "\n",
    "# Generate the grid of points for XYZ\n",
    "N = 35\n",
    "scale = 1.5\n",
    "x = torch.linspace(-scale, scale, N, device=device)\n",
    "y = torch.linspace(-scale, scale, N, device=device)\n",
    "z = torch.linspace(-scale, scale, N, device=device)\n",
    "x, y, z = torch.meshgrid((x, y, z))\n",
    "xyz = torch.cat((x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)), dim=1).to(device)  # [N^3, 3]\n",
    "xyz.requires_grad_(True)  # Ensure gradients for density gradient computation\n",
    "\n",
    "# Define a view direction for color queries (e.g., along the z-axis)\n",
    "view_direction = torch.tensor([0, 0, 1], device=device).expand(xyz.shape[0], -1)\n",
    "\n",
    "# Query the NeRF model for colors and density\n",
    "rgb, density = nerf_model.forward(xyz, view_direction)  # RGB: [N^3, 3], Density: [N^3]\n",
    "\n",
    "# Compute RGB gradients across points\n",
    "rgb_gradient = np.linalg.norm(np.gradient(rgb.cpu().detach().numpy(), axis=0), axis=-1)  # Approximate RGB gradient\n",
    "\n",
    "# Compute density gradients across points\n",
    "density_gradient = torch.autograd.grad(\n",
    "    outputs=density,\n",
    "    inputs=xyz,\n",
    "    grad_outputs=torch.ones_like(density),  # Same shape as density\n",
    "    create_graph=True,\n",
    ")[0]  # [N^3, 3]\n",
    "density_gradient_magnitude = torch.norm(density_gradient, dim=-1).cpu().detach().numpy()\n",
    "\n",
    "# Define thresholds\n",
    "rgb_gradient_threshold = np.percentile(rgb_gradient, 90)  # Keep top 10% of RGB gradients\n",
    "density_gradient_threshold = np.percentile(density_gradient_magnitude, 90)  # Top 10% of density gradients\n",
    "\n",
    "# Apply combined mask\n",
    "rgb_gradient_mask = rgb_gradient >= rgb_gradient_threshold\n",
    "density_gradient_mask = density_gradient_magnitude >= density_gradient_threshold\n",
    "combined_mask = rgb_gradient_mask & density_gradient_mask\n",
    "\n",
    "# Filter points and attributes based on the combined mask\n",
    "filtered_xyz = xyz[combined_mask].cpu().detach().numpy()\n",
    "filtered_rgb = rgb[combined_mask].cpu().detach().numpy()  # Filter RGB values\n",
    "filtered_density = density[combined_mask].cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# Compute the variogram for color\n",
    "color_variogram = Variogram(\n",
    "    filtered_xyz, \n",
    "    filtered_rgb.mean(axis=1),  # Use mean intensity as a scalar representation of color\n",
    "    model='spherical', \n",
    "    normalize=False, \n",
    "    nugget=0.1\n",
    ")\n",
    "\n",
    "\n",
    "# Describe the fitted variogram\n",
    "print(color_variogram.describe())\n",
    "\n",
    "# Plot the variogram\n",
    "color_variogram.plot()\n",
    "plt.title(\"3D Variogram for Filtered NeRF Colors\")\n",
    "plt.show()\n",
    "\n",
    "# Compute pairwise distances between points\n",
    "pairwise_distances = squareform(pdist(filtered_xyz))  # [N_filtered, N_filtered]\n",
    "\n",
    "# Get the bin edges and experimental variogram values\n",
    "bin_edges = color_variogram.bins  # Bin edges (distance ranges)\n",
    "bin_uncertainties = color_variogram.experimental  # Semi-variance per bin\n",
    "\n",
    "# Initialize uncertainties for each point\n",
    "point_uncertainties = np.zeros(filtered_xyz.shape[0])\n",
    "\n",
    "# Assign uncertainties based on the average bin of the point's neighbors\n",
    "for i in range(filtered_xyz.shape[0]):\n",
    "    # Find distances from the current point\n",
    "    distances = pairwise_distances[i]\n",
    "    \n",
    "    # Find which bin each distance falls into\n",
    "    bin_indices = np.digitize(distances, bin_edges, right=True)\n",
    "    \n",
    "    # Assign the uncertainty as the average uncertainty of its neighbors' bins\n",
    "    neighbor_uncertainties = bin_uncertainties[bin_indices - 1]  # Adjust index\n",
    "    point_uncertainties[i] = np.mean(neighbor_uncertainties)\n",
    "\n",
    "# Normalize point uncertainties\n",
    "point_uncertainties = (point_uncertainties - np.min(point_uncertainties)) / (\n",
    "    np.max(point_uncertainties) - np.min(point_uncertainties)\n",
    ")\n",
    "\n",
    "# Assign colors based on uncertainty using a colormap\n",
    "colormap = cm.get_cmap('inferno')\n",
    "colors = colormap(point_uncertainties)[:, :3]  # Get RGB values\n",
    "\n",
    "# Define an uncertainty threshold for spheres\n",
    "threshold_uncertainty = 0.8  # Adjust this value to filter spheres\n",
    "threshold_mask = point_uncertainties >= threshold_uncertainty\n",
    "filtered_xyz_thresholded = filtered_xyz[threshold_mask]\n",
    "point_uncertainties_thresholded = point_uncertainties[threshold_mask]\n",
    "colors_thresholded = colors[threshold_mask]\n",
    "sphere_sizes_thresholded = 0.05 + point_uncertainties_thresholded * 0.001  # Adjust size based on uncertainty\n",
    "\n",
    "# Create spheres for the filtered points\n",
    "spheres = []\n",
    "for point, size, color in zip(filtered_xyz_thresholded, sphere_sizes_thresholded, colors_thresholded):\n",
    "    sphere = trimesh.primitives.Sphere(\n",
    "        radius=size, center=point, subdivisions=2  # Subdivisions for smoothness\n",
    "    )\n",
    "    # Apply the color to the sphere\n",
    "    sphere.visual.vertex_colors = (color * 255).astype(np.uint8)\n",
    "    spheres.append(sphere)\n",
    "\n",
    "# Add the mesh for spatial context\n",
    "density_np = density.cpu().detach().numpy().reshape(N, N, N)  # Use the full density array\n",
    "vertices, triangles = mcubes.marching_cubes(density_np, 3 * np.mean(density_np))  # Adjust threshold as needed\n",
    "vertices_scaled = (vertices / N) * (2 * scale) - scale\n",
    "mesh = trimesh.Trimesh(vertices_scaled, triangles)\n",
    "\n",
    "# Combine the mesh and spheres into a single scene\n",
    "scene = trimesh.Scene([mesh] + spheres)\n",
    "\n",
    "# Show the combined scene\n",
    "scene.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capturing uncertainty averaged across 3 viewing direction for RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import torch\n",
    "import mcubes\n",
    "import numpy as np\n",
    "from skgstat import Variogram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "device = 'cuda'\n",
    "pth_file = 'experiments/suzanne/set100/models/M0.pth'\n",
    "nerf_model = torch.load(pth_file).to(device)\n",
    "\n",
    "# Generate the grid of points for XYZ\n",
    "N = 35\n",
    "scale = 1.5\n",
    "x = torch.linspace(-scale, scale, N, device=device)\n",
    "y = torch.linspace(-scale, scale, N, device=device)\n",
    "z = torch.linspace(-scale, scale, N, device=device)\n",
    "x, y, z = torch.meshgrid((x, y, z))\n",
    "xyz = torch.cat((x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)), dim=1).to(device)  # [N^3, 3]\n",
    "\n",
    "# Define multiple view directions for averaging\n",
    "view_directions = torch.tensor([[0, 0, 1], [1, 0, 0], [0, 1, 0]], device=device)  # Example directions\n",
    "averaged_rgb = torch.zeros(xyz.shape[0], 3, device=device)  # Initialize for averaging\n",
    "\n",
    "for direction in view_directions:\n",
    "    rgb, _ = nerf_model.forward(xyz, direction.expand(xyz.shape[0], -1))\n",
    "    averaged_rgb += rgb  # Accumulate RGB values\n",
    "\n",
    "averaged_rgb /= len(view_directions)  # Compute the average RGB\n",
    "\n",
    "# Convert to numpy for processing\n",
    "xyz = xyz.cpu().detach().numpy()\n",
    "averaged_rgb = averaged_rgb.cpu().detach().numpy()\n",
    "\n",
    "# Apply masking to reduce the number of points\n",
    "threshold_value = np.quantile(averaged_rgb.mean(axis=1), 0.5)  # Use median RGB intensity as threshold\n",
    "mask = averaged_rgb.mean(axis=1) >= threshold_value  # Mask based on averaged RGB intensity\n",
    "\n",
    "# Filter points and colors using the mask\n",
    "filtered_xyz = xyz[mask]\n",
    "filtered_rgb = averaged_rgb[mask]\n",
    "\n",
    "# Compute the variogram for color\n",
    "color_variogram = Variogram(\n",
    "    filtered_xyz,\n",
    "    filtered_rgb.mean(axis=1),  # Use mean intensity as a scalar representation of color\n",
    "    model='spherical',\n",
    "    normalize=False,\n",
    "    nugget=0.1\n",
    ")\n",
    "\n",
    "# Describe the fitted variogram\n",
    "print(color_variogram.describe())\n",
    "\n",
    "# Plot the variogram\n",
    "color_variogram.plot()\n",
    "plt.title(\"3D Variogram for Filtered NeRF Colors (Averaged RGB)\")\n",
    "plt.show()\n",
    "\n",
    "# Compute pairwise distances between points\n",
    "pairwise_distances = squareform(pdist(filtered_xyz))  # [N_filtered, N_filtered]\n",
    "\n",
    "# Get the bin edges and experimental variogram values\n",
    "bin_edges = color_variogram.bins  # Bin edges (distance ranges)\n",
    "bin_uncertainties = color_variogram.experimental  # Semi-variance per bin\n",
    "\n",
    "# Initialize uncertainties for each point\n",
    "point_uncertainties = np.zeros(filtered_xyz.shape[0])\n",
    "\n",
    "# Assign uncertainties based on the average bin of the point's neighbors\n",
    "for i in range(filtered_xyz.shape[0]):\n",
    "    distances = pairwise_distances[i]\n",
    "    bin_indices = np.digitize(distances, bin_edges, right=True)\n",
    "    neighbor_uncertainties = bin_uncertainties[bin_indices - 1]  # Adjust index\n",
    "    point_uncertainties[i] = np.mean(neighbor_uncertainties)\n",
    "\n",
    "# Normalize point uncertainties\n",
    "point_uncertainties = (point_uncertainties - np.min(point_uncertainties)) / (\n",
    "    np.max(point_uncertainties) - np.min(point_uncertainties)\n",
    ")\n",
    "\n",
    "# Assign colors based on uncertainty using a colormap\n",
    "colormap = cm.get_cmap('inferno')\n",
    "colors = colormap(point_uncertainties)[:, :3]  # Get RGB values\n",
    "\n",
    "#    mesh = trimesh.Trimesh(vertices_scaled, triangles)\n",
    "def update_scene(threshold):\n",
    "    # Apply dynamic threshold\n",
    "    threshold_mask = point_uncertainties >= threshold\n",
    "    filtered_xyz_thresholded = filtered_xyz[threshold_mask]\n",
    "    point_uncertainties_thresholded = point_uncertainties[threshold_mask]\n",
    "    colors_thresholded = colors[threshold_mask]\n",
    "\n",
    "    # Define sphere sizes for the remaining points\n",
    "    sphere_sizes_thresholded = 0.05 + point_uncertainties_thresholded * 0.001\n",
    "\n",
    "    # Create spheres for the thresholded points\n",
    "    spheres = []\n",
    "    for point, size, color in zip(filtered_xyz_thresholded, sphere_sizes_thresholded, colors_thresholded):\n",
    "        sphere = trimesh.primitives.Sphere(\n",
    "            radius=size, center=point, subdivisions=2  # Subdivisions for smoothness\n",
    "        )\n",
    "        sphere.visual.vertex_colors = (color * 255).astype(np.uint8)\n",
    "        spheres.append(sphere)\n",
    "\n",
    "    # Add the mesh for spatial context\n",
    "    density_np = averaged_rgb.mean(axis=1).reshape(N, N, N)  # Use the full averaged RGB for mesh visualization\n",
    "    vertices, triangles = mcubes.marching_cubes(density_np, 3 * np.mean(density_np))\n",
    "    vertices_scaled = (vertices / N) * (2 * scale) - scale\n",
    "    mesh = trimesh.Trimesh(vertices_scaled, triangles)\n",
    "\n",
    "    # Combine the mesh and spheres into a single scene\n",
    "    scene = trimesh.Scene([mesh] + spheres)\n",
    "\n",
    "    # Show the scene\n",
    "    scene.export(\"scene.glb\")  # Save as a GLB file\n",
    "\n",
    "\n",
    "\n",
    "# Create an interactive slider to adjust the threshold\n",
    "interact(update_scene, threshold=FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying 5 viewing directions to average, the top down, and 4 evenly spaced canoncial views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import torch\n",
    "import mcubes\n",
    "import numpy as np\n",
    "from skgstat import Variogram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "device = 'cuda'\n",
    "pth_file = 'experiments/suzanne/set100/models/M0.pth'\n",
    "nerf_model = torch.load(pth_file).to(device)\n",
    "\n",
    "# Generate the grid of points for XYZ\n",
    "N = 35\n",
    "scale = 1.5\n",
    "x = torch.linspace(-scale, scale, N, device=device)\n",
    "y = torch.linspace(-scale, scale, N, device=device)\n",
    "z = torch.linspace(-scale, scale, N, device=device)\n",
    "x, y, z = torch.meshgrid((x, y, z))\n",
    "xyz = torch.cat((x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)), dim=1).to(device)  # [N^3, 3]\n",
    "\n",
    "# Define multiple view directions for averaging\n",
    "view_directions = torch.tensor([\n",
    "    [0, 0, 1],             # Top-down\n",
    "    [0, 0.5, 0.87],        # Front-right\n",
    "    [0.87, 0, 0.5],        # Front-left\n",
    "    [0, -0.5, 0.87],       # Back-right\n",
    "    [-0.87, 0, 0.5]        # Back-left\n",
    "], device=device)\n",
    "\n",
    "averaged_rgb = torch.zeros(xyz.shape[0], 3, device=device)  # Initialize for averaging\n",
    "\n",
    "for direction in view_directions:\n",
    "    rgb, _ = nerf_model.forward(xyz, direction.expand(xyz.shape[0], -1))\n",
    "    averaged_rgb += rgb  # Accumulate RGB values\n",
    "\n",
    "averaged_rgb /= len(view_directions)  # Compute the average RGB\n",
    "\n",
    "# Convert to numpy for processing\n",
    "xyz = xyz.cpu().detach().numpy()\n",
    "averaged_rgb = averaged_rgb.cpu().detach().numpy()\n",
    "\n",
    "# Apply masking to reduce the number of points\n",
    "threshold_value = np.quantile(averaged_rgb.mean(axis=1), 0.5)  # Use median RGB intensity as threshold\n",
    "mask = averaged_rgb.mean(axis=1) >= threshold_value  # Mask based on averaged RGB intensity\n",
    "\n",
    "# Filter points and colors using the mask\n",
    "filtered_xyz = xyz[mask]\n",
    "filtered_rgb = averaged_rgb[mask]\n",
    "\n",
    "# Compute the variogram for color\n",
    "color_variogram = Variogram(\n",
    "    filtered_xyz,\n",
    "    filtered_rgb.mean(axis=1),  # Use mean intensity as a scalar representation of color\n",
    "    model='spherical',\n",
    "    normalize=False,\n",
    "    nugget=0.1\n",
    ")\n",
    "\n",
    "# Describe the fitted variogram\n",
    "print(color_variogram.describe())\n",
    "\n",
    "# Plot the variogram\n",
    "color_variogram.plot()\n",
    "plt.title(\"3D Variogram for Filtered NeRF Colors (Averaged RGB)\")\n",
    "plt.show()\n",
    "\n",
    "# Compute pairwise distances between points\n",
    "pairwise_distances = squareform(pdist(filtered_xyz))  # [N_filtered, N_filtered]\n",
    "\n",
    "# Get the bin edges and experimental variogram values\n",
    "bin_edges = color_variogram.bins  # Bin edges (distance ranges)\n",
    "bin_uncertainties = color_variogram.experimental  # Semi-variance per bin\n",
    "\n",
    "# Initialize uncertainties for each point\n",
    "point_uncertainties = np.zeros(filtered_xyz.shape[0])\n",
    "\n",
    "# Assign uncertainties based on the average bin of the point's neighbors\n",
    "for i in range(filtered_xyz.shape[0]):\n",
    "    distances = pairwise_distances[i]\n",
    "    bin_indices = np.digitize(distances, bin_edges, right=True)\n",
    "    neighbor_uncertainties = bin_uncertainties[bin_indices - 1]  # Adjust index\n",
    "    point_uncertainties[i] = np.mean(neighbor_uncertainties)\n",
    "\n",
    "# Normalize point uncertainties\n",
    "point_uncertainties = (point_uncertainties - np.min(point_uncertainties)) / (\n",
    "    np.max(point_uncertainties) - np.min(point_uncertainties)\n",
    ")\n",
    "\n",
    "# Assign colors based on uncertainty using a colormap\n",
    "colormap = cm.get_cmap('inferno')\n",
    "colors = colormap(point_uncertainties)[:, :3]  # Get RGB values\n",
    "\n",
    "#    mesh = trimesh.Trimesh(vertices_scaled, triangles)\n",
    "def update_scene(threshold):\n",
    "    # Apply dynamic threshold\n",
    "    threshold_mask = point_uncertainties >= threshold\n",
    "    filtered_xyz_thresholded = filtered_xyz[threshold_mask]\n",
    "    point_uncertainties_thresholded = point_uncertainties[threshold_mask]\n",
    "    colors_thresholded = colors[threshold_mask]\n",
    "\n",
    "    # Define sphere sizes for the remaining points\n",
    "    sphere_sizes_thresholded = 0.05 + point_uncertainties_thresholded * 0.001\n",
    "\n",
    "    # Create spheres for the thresholded points\n",
    "    spheres = []\n",
    "    for point, size, color in zip(filtered_xyz_thresholded, sphere_sizes_thresholded, colors_thresholded):\n",
    "        sphere = trimesh.primitives.Sphere(\n",
    "            radius=size, center=point, subdivisions=2  # Subdivisions for smoothness\n",
    "        )\n",
    "        sphere.visual.vertex_colors = (color * 255).astype(np.uint8)\n",
    "        spheres.append(sphere)\n",
    "\n",
    "    # Add the mesh for spatial context\n",
    "    density_np = averaged_rgb.mean(axis=1).reshape(N, N, N)  # Use the full averaged RGB for mesh visualization\n",
    "    vertices, triangles = mcubes.marching_cubes(density_np, 3 * np.mean(density_np))\n",
    "    vertices_scaled = (vertices / N) * (2 * scale) - scale\n",
    "    mesh = trimesh.Trimesh(vertices_scaled, triangles)\n",
    "\n",
    "    # Combine the mesh and spheres into a single scene\n",
    "    scene = trimesh.Scene([mesh] + spheres)\n",
    "\n",
    "    # Show the scene\n",
    "    scene.export(\"scene.glb\")  # Save as a GLB file\n",
    "\n",
    "\n",
    "\n",
    "# Create an interactive slider to adjust the threshold\n",
    "interact(update_scene, threshold=FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding lines to view from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import torch\n",
    "import mcubes\n",
    "import numpy as np\n",
    "from skgstat import Variogram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "device = 'cuda'\n",
    "pth_file = 'experiments/suzanne/set100/models/M0.pth'\n",
    "nerf_model = torch.load(pth_file).to(device)\n",
    "\n",
    "# Generate the grid of points for XYZ\n",
    "N = 35\n",
    "scale = 1.5\n",
    "x = torch.linspace(-scale, scale, N, device=device)\n",
    "y = torch.linspace(-scale, scale, N, device=device)\n",
    "z = torch.linspace(-scale, scale, N, device=device)\n",
    "x, y, z = torch.meshgrid((x, y, z))\n",
    "xyz = torch.cat((x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)), dim=1).to(device)\n",
    "\n",
    "# Define 5 canonical view directions: top-down and perspectives\n",
    "view_directions = torch.tensor([\n",
    "    [0, 0, 1],             # Top-down\n",
    "    [0, 0.5, 0.87],        # Front-right\n",
    "    [0.87, 0, 0.5],        # Front-left\n",
    "    [0, -0.5, 0.87],       # Back-right\n",
    "    [-0.87, 0, 0.5]        # Back-left\n",
    "], device=device)\n",
    "\n",
    "# Compute averaged RGB from 5 views\n",
    "averaged_rgb = torch.zeros(xyz.shape[0], 3, device=device)\n",
    "for direction in view_directions:\n",
    "    rgb, _ = nerf_model.forward(xyz, direction.expand(xyz.shape[0], -1))\n",
    "    averaged_rgb += rgb\n",
    "\n",
    "averaged_rgb /= len(view_directions)  # Average RGB\n",
    "\n",
    "# Convert to numpy for further processing\n",
    "xyz = xyz.cpu().detach().numpy()\n",
    "averaged_rgb = averaged_rgb.cpu().detach().numpy()\n",
    "\n",
    "# Apply masking based on averaged RGB intensity\n",
    "threshold_value = np.quantile(averaged_rgb.mean(axis=1), 0.5)\n",
    "mask = averaged_rgb.mean(axis=1) >= threshold_value\n",
    "\n",
    "# Filter points and RGB values\n",
    "filtered_xyz = xyz[mask]\n",
    "filtered_rgb = averaged_rgb[mask]\n",
    "\n",
    "# Compute the variogram for color\n",
    "color_variogram = Variogram(\n",
    "    filtered_xyz,\n",
    "    filtered_rgb.mean(axis=1),\n",
    "    model='spherical',\n",
    "    normalize=False,\n",
    "    nugget=0.1\n",
    ")\n",
    "\n",
    "# Compute uncertainties\n",
    "pairwise_distances = squareform(pdist(filtered_xyz))\n",
    "bin_edges = color_variogram.bins\n",
    "bin_uncertainties = color_variogram.experimental\n",
    "point_uncertainties = np.zeros(filtered_xyz.shape[0])\n",
    "\n",
    "for i in range(filtered_xyz.shape[0]):\n",
    "    distances = pairwise_distances[i]\n",
    "    bin_indices = np.digitize(distances, bin_edges, right=True)\n",
    "    neighbor_uncertainties = bin_uncertainties[bin_indices - 1]\n",
    "    point_uncertainties[i] = np.mean(neighbor_uncertainties)\n",
    "\n",
    "# Normalize uncertainties\n",
    "point_uncertainties = (point_uncertainties - np.min(point_uncertainties)) / (\n",
    "    np.max(point_uncertainties) - np.min(point_uncertainties)\n",
    ")\n",
    "\n",
    "# Assign colors based on uncertainty using a colormap\n",
    "colormap = cm.get_cmap('inferno')\n",
    "colors = colormap(point_uncertainties)[:, :3]  # Get RGB values\n",
    "\n",
    "def update_scene(threshold):\n",
    "    # Apply dynamic threshold\n",
    "    threshold_mask = point_uncertainties >= threshold\n",
    "    filtered_xyz_thresholded = filtered_xyz[threshold_mask]\n",
    "    point_uncertainties_thresholded = point_uncertainties[threshold_mask]\n",
    "    colors_thresholded = colors[threshold_mask]\n",
    "\n",
    "    # Define sphere sizes for the remaining points\n",
    "    sphere_sizes_thresholded = 0.05 + point_uncertainties_thresholded * 0.001\n",
    "\n",
    "    # Create spheres for the thresholded points\n",
    "    spheres = []\n",
    "    for point, size, color in zip(filtered_xyz_thresholded, sphere_sizes_thresholded, colors_thresholded):\n",
    "        sphere = trimesh.primitives.Sphere(\n",
    "            radius=size, center=point, subdivisions=2  # Subdivisions for smoothness\n",
    "        )\n",
    "        sphere.visual.vertex_colors = (color * 255).astype(np.uint8)\n",
    "        spheres.append(sphere)\n",
    "\n",
    "    # Add mesh for spatial context\n",
    "    density_np = averaged_rgb.mean(axis=1).reshape(N, N, N)  # Use the full averaged RGB for mesh visualization\n",
    "    vertices, triangles = mcubes.marching_cubes(density_np, 3 * np.mean(density_np))\n",
    "    vertices_scaled = (vertices / N) * (2 * scale) - scale\n",
    "    mesh = trimesh.Trimesh(vertices_scaled, triangles)\n",
    "\n",
    "    # Draw view direction vectors\n",
    "    center = np.array([0, 0, 0])  # Assume object is centered at the origin\n",
    "    view_lines = []\n",
    "    for direction in view_directions.cpu().numpy():\n",
    "        arrow_start = center\n",
    "        arrow_end = center + 3 * direction  # Make vectors 3x longer\n",
    "        line = trimesh.load_path(np.array([arrow_start, arrow_end]))\n",
    "        \n",
    "        # Assign red color to the path\n",
    "        line_colors = np.array([[255, 0, 0, 255]] * len(line.entities))  # RGBA for red, fully opaque\n",
    "        line.colors = line_colors  # Assign per-entity colors\n",
    "        line.width = 2.0  # Set line thickness\n",
    "        view_lines.append(line)\n",
    "\n",
    "    # Combine the mesh, spheres, and view vectors into a single scene\n",
    "    scene = trimesh.Scene([mesh] + spheres + view_lines)\n",
    "\n",
    "    # Show the scene\n",
    "    scene.export(\"scene.glb\")\n",
    "\n",
    "# Create an interactive slider to adjust the threshold\n",
    "interact(update_scene, threshold=FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# true canonical persepective for views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import torch\n",
    "import mcubes\n",
    "import numpy as np\n",
    "from skgstat import Variogram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import math\n",
    "\n",
    "device = 'cuda'\n",
    "pth_file = 'experiments/suzanne/set100/models/M0.pth'\n",
    "nerf_model = torch.load(pth_file).to(device)\n",
    "\n",
    "# Generate the grid of points for XYZ\n",
    "N = 35\n",
    "scale = 1.5\n",
    "x = torch.linspace(-scale, scale, N, device=device)\n",
    "y = torch.linspace(-scale, scale, N, device=device)\n",
    "z = torch.linspace(-scale, scale, N, device=device)\n",
    "x, y, z = torch.meshgrid((x, y, z))\n",
    "xyz = torch.cat((x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)), dim=1).to(device)\n",
    "\n",
    "# Canonical perspective view directions\n",
    "view_directions = torch.tensor([\n",
    "    [0, 0, 1],  # Top-down\n",
    "    [math.cos(math.radians(45)) * math.cos(math.radians(30)), \n",
    "     math.sin(math.radians(45)) * math.cos(math.radians(30)), \n",
    "     math.sin(math.radians(30))],  # Front-right (30° elevation, 45° azimuth)\n",
    "    [math.cos(math.radians(-45)) * math.cos(math.radians(30)), \n",
    "     math.sin(math.radians(-45)) * math.cos(math.radians(30)), \n",
    "     math.sin(math.radians(30))],  # Front-left (30° elevation, -45° azimuth)\n",
    "    [math.cos(math.radians(135)) * math.cos(math.radians(30)), \n",
    "     math.sin(math.radians(135)) * math.cos(math.radians(30)), \n",
    "     math.sin(math.radians(30))],  # Back-right (30° elevation, 135° azimuth)\n",
    "    [math.cos(math.radians(-135)) * math.cos(math.radians(30)), \n",
    "     math.sin(math.radians(-135)) * math.cos(math.radians(30)), \n",
    "     math.sin(math.radians(30))]   # Back-left (30° elevation, -135° azimuth)\n",
    "], device=device)\n",
    "\n",
    "\n",
    "# Compute averaged RGB from 5 views\n",
    "averaged_rgb = torch.zeros(xyz.shape[0], 3, device=device)\n",
    "for direction in view_directions:\n",
    "    rgb, _ = nerf_model.forward(xyz, direction.expand(xyz.shape[0], -1))\n",
    "    averaged_rgb += rgb\n",
    "\n",
    "averaged_rgb /= len(view_directions)  # Average RGB\n",
    "\n",
    "# Convert to numpy for further processing\n",
    "xyz = xyz.cpu().detach().numpy()\n",
    "averaged_rgb = averaged_rgb.cpu().detach().numpy()\n",
    "\n",
    "# Apply masking based on averaged RGB intensity\n",
    "threshold_value = np.quantile(averaged_rgb.mean(axis=1), 0.5)\n",
    "mask = averaged_rgb.mean(axis=1) >= threshold_value\n",
    "\n",
    "# Filter points and RGB values\n",
    "filtered_xyz = xyz[mask]\n",
    "filtered_rgb = averaged_rgb[mask]\n",
    "\n",
    "# Compute the variogram for color\n",
    "color_variogram = Variogram(\n",
    "    filtered_xyz,\n",
    "    filtered_rgb.mean(axis=1),\n",
    "    model='spherical',\n",
    "    normalize=False,\n",
    "    nugget=0.1\n",
    ")\n",
    "\n",
    "# Compute uncertainties\n",
    "pairwise_distances = squareform(pdist(filtered_xyz))\n",
    "bin_edges = color_variogram.bins\n",
    "bin_uncertainties = color_variogram.experimental\n",
    "point_uncertainties = np.zeros(filtered_xyz.shape[0])\n",
    "\n",
    "for i in range(filtered_xyz.shape[0]):\n",
    "    distances = pairwise_distances[i]\n",
    "    bin_indices = np.digitize(distances, bin_edges, right=True)\n",
    "    neighbor_uncertainties = bin_uncertainties[bin_indices - 1]\n",
    "    point_uncertainties[i] = np.mean(neighbor_uncertainties)\n",
    "\n",
    "# Normalize uncertainties\n",
    "point_uncertainties = (point_uncertainties - np.min(point_uncertainties)) / (\n",
    "    np.max(point_uncertainties) - np.min(point_uncertainties)\n",
    ")\n",
    "\n",
    "# Assign colors based on uncertainty using a colormap\n",
    "colormap = cm.get_cmap('inferno')\n",
    "colors = colormap(point_uncertainties)[:, :3]  # Get RGB values\n",
    "\n",
    "def update_scene(threshold):\n",
    "    # Apply dynamic threshold\n",
    "    threshold_mask = point_uncertainties >= threshold\n",
    "    filtered_xyz_thresholded = filtered_xyz[threshold_mask]\n",
    "    point_uncertainties_thresholded = point_uncertainties[threshold_mask]\n",
    "    colors_thresholded = colors[threshold_mask]\n",
    "\n",
    "    # Define sphere sizes for the remaining points\n",
    "    sphere_sizes_thresholded = 0.05 + point_uncertainties_thresholded * 0.001\n",
    "\n",
    "    # Create spheres for the thresholded points\n",
    "    spheres = []\n",
    "    for point, size, color in zip(filtered_xyz_thresholded, sphere_sizes_thresholded, colors_thresholded):\n",
    "        sphere = trimesh.primitives.Sphere(\n",
    "            radius=size, center=point, subdivisions=2  # Subdivisions for smoothness\n",
    "        )\n",
    "        sphere.visual.vertex_colors = (color * 255).astype(np.uint8)\n",
    "        spheres.append(sphere)\n",
    "\n",
    "    # Add mesh for spatial context\n",
    "    density_np = averaged_rgb.mean(axis=1).reshape(N, N, N)  # Use the full averaged RGB for mesh visualization\n",
    "    vertices, triangles = mcubes.marching_cubes(density_np, 3 * np.mean(density_np))\n",
    "    vertices_scaled = (vertices / N) * (2 * scale) - scale\n",
    "    mesh = trimesh.Trimesh(vertices_scaled, triangles)\n",
    "\n",
    "    # Draw view direction vectors\n",
    "    center = np.array([0, 0, 0])  # Assume object is centered at the origin\n",
    "    view_lines = []\n",
    "    for direction in view_directions.cpu().numpy():\n",
    "        arrow_start = center\n",
    "        arrow_end = center + 3 * direction  # Make vectors 3x longer\n",
    "        line = trimesh.load_path(np.array([arrow_start, arrow_end]))\n",
    "        \n",
    "        # Assign red color to the path\n",
    "        line_colors = np.array([[255, 0, 0, 255]] * len(line.entities))  # RGBA for red, fully opaque\n",
    "        line.colors = line_colors  # Assign per-entity colors\n",
    "        line.width = 2.0  # Set line thickness\n",
    "        view_lines.append(line)\n",
    "\n",
    "    # Combine the mesh, spheres, and view vectors into a single scene\n",
    "    scene = trimesh.Scene([mesh] + spheres + view_lines)\n",
    "\n",
    "    # Show the scene\n",
    "    scene.export(\"scene.glb\")\n",
    "\n",
    "# Create an interactive slider to adjust the threshold\n",
    "interact(update_scene, threshold=FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nov 25th: testing assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import torch\n",
    "import mcubes\n",
    "import numpy as np\n",
    "from skgstat import Variogram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "device = 'cuda'\n",
    "pth_file = 'experiments/suzanne/set100/models/M0.pth'\n",
    "# pth_file = \"experiments/test/set30/models/M0.pth\"\n",
    "nerf_model = torch.load(pth_file).to(device)\n",
    "\n",
    "# Generate the grid of points for XYZ\n",
    "N = 35\n",
    "scale = 1.5\n",
    "x = torch.linspace(-scale, scale, N, device=device)\n",
    "y = torch.linspace(-scale, scale, N, device=device)\n",
    "z = torch.linspace(-scale, scale, N, device=device)\n",
    "x, y, z = torch.meshgrid((x, y, z))\n",
    "xyz = torch.cat((x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)), dim=1)  # [125000, 3] points\n",
    "\n",
    "# Define a view direction for color queries (e.g., along the z-axis)\n",
    "view_direction = torch.tensor([0, 0, 1], device=device).expand(xyz.shape[0], -1)\n",
    "\n",
    "# Query the NeRF model for colors (assumes `forward` outputs RGB and density)\n",
    "rgb, _ = nerf_model.forward(xyz, view_direction)  # [125000, 3]\n",
    "\n",
    "# Convert to numpy for processing\n",
    "xyz = xyz.cpu().detach().numpy()\n",
    "rgb = rgb.cpu().detach().numpy()\n",
    "\n",
    "# Apply masking to reduce the number of points\n",
    "threshold_value = np.quantile(rgb.mean(axis=1), 0.5)  # Use median RGB intensity as threshold\n",
    "mask = rgb.mean(axis=1) >= threshold_value  # Mask based only on RGB intensity\n",
    "\n",
    "# Filter points and colors using the mask\n",
    "filtered_xyz = xyz[mask]\n",
    "filtered_rgb = rgb[mask]\n",
    "\n",
    "# Compute the variogram for color\n",
    "color_variogram = Variogram(\n",
    "    filtered_xyz, \n",
    "    filtered_rgb.mean(axis=1),  # Use mean intensity as a scalar representation of color\n",
    "    model='spherical', \n",
    "    normalize=False, \n",
    "    nugget=0.1\n",
    ")\n",
    "\n",
    "# Describe the fitted variogram\n",
    "print(color_variogram.describe())\n",
    "\n",
    "# Plot the variogram\n",
    "color_variogram.plot()\n",
    "plt.title(\"3D Variogram for Filtered NeRF Colors\")\n",
    "plt.show()\n",
    "\n",
    "# Compute pairwise distances between points\n",
    "pairwise_distances = squareform(pdist(filtered_xyz))  # [N, N]\n",
    "\n",
    "# Get the bin edges and experimental variogram values\n",
    "bin_edges = color_variogram.bins  # Bin edges (distance ranges)\n",
    "bin_uncertainties = color_variogram.experimental  # Semi-variance per bin\n",
    "\n",
    "# Initialize uncertainties for each point\n",
    "point_uncertainties = np.zeros(filtered_xyz.shape[0])\n",
    "\n",
    "# Assign uncertainties based on the average bin of the point's neighbors\n",
    "for i in range(filtered_xyz.shape[0]):\n",
    "    # Find distances from the current point\n",
    "    distances = pairwise_distances[i]\n",
    "    \n",
    "    # Find which bin each distance falls into\n",
    "    bin_indices = np.digitize(distances, bin_edges, right=True)\n",
    "    \n",
    "    # Assign the uncertainty as the average uncertainty of its neighbors' bins\n",
    "    neighbor_uncertainties = bin_uncertainties[bin_indices - 1]  # Adjust index\n",
    "    point_uncertainties[i] = np.mean(neighbor_uncertainties)\n",
    "\n",
    "# Normalize point uncertainties\n",
    "point_uncertainties = (point_uncertainties - np.min(point_uncertainties)) / (\n",
    "    np.max(point_uncertainties) - np.min(point_uncertainties)\n",
    ")\n",
    "\n",
    "# Assign colors based on uncertainty using a colormap\n",
    "colormap = cm.get_cmap('inferno')\n",
    "colors = colormap(point_uncertainties)[:, :3]  # Get RGB values\n",
    "\n",
    "# Create a PointCloud with colors\n",
    "point_cloud = trimesh.points.PointCloud(filtered_xyz, colors=colors)\n",
    "\n",
    "# Define the uncertainty threshold (e.g., 0.5)\n",
    "threshold = 0.99\n",
    "\n",
    "# Filter points, uncertainties, and colors based on the threshold\n",
    "threshold_mask = point_uncertainties >= threshold  # Keep points with uncertainty >= threshold\n",
    "filtered_xyz_thresholded = filtered_xyz[threshold_mask]\n",
    "point_uncertainties_thresholded = point_uncertainties[threshold_mask]\n",
    "colors_thresholded = colors[threshold_mask]\n",
    "\n",
    "# Define sphere sizes for the remaining points\n",
    "sphere_sizes_thresholded = 0.05 + point_uncertainties_thresholded * 0.001\n",
    "# Create spheres for the thresholded points\n",
    "spheres = []\n",
    "for point, size, color in zip(filtered_xyz_thresholded, sphere_sizes_thresholded, colors_thresholded):\n",
    "    sphere = trimesh.primitives.Sphere(\n",
    "        radius=size, center=point, subdivisions=2  # Subdivisions for smoothness\n",
    "    )\n",
    "    # Apply the color to the sphere\n",
    "    sphere.visual.vertex_colors = (color * 255).astype(np.uint8)\n",
    "    spheres.append(sphere)\n",
    "\n",
    "# Add the spheres and the mesh to the scene\n",
    "density_np = rgb.mean(axis=1).reshape(N, N, N)  # Example for visualization\n",
    "vertices, triangles = mcubes.marching_cubes(density_np, 3 * np.mean(density_np))\n",
    "vertices_scaled = (vertices / N) * (2 * scale) - scale\n",
    "mesh = trimesh.Trimesh(vertices_scaled, triangles)\n",
    "scene = trimesh.Scene([mesh] + spheres)\n",
    "\n",
    "# Show the scene\n",
    "scene.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nov 25: experimenting with the variogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Generate synthetic point cloud data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Region 1: Gaussian with mean 0, variance 1\n",
    "N1 = 200\n",
    "points_low_var = np.random.rand(N1, 2) * 50  # Points in lower-left region\n",
    "values_low_var = np.random.normal(0, 1, N1)  # Scalar values\n",
    "\n",
    "# Region 2: Gaussian with mean 0, variance 5 (upper-right region)\n",
    "N2 = 50\n",
    "points_high_var = np.random.rand(N2, 2) * 20 + 50  # Points in upper-right region\n",
    "values_high_var = np.random.normal(0, np.sqrt(5), N2)  # Scalar values\n",
    "\n",
    "# Combine both regions\n",
    "points = np.vstack([points_low_var, points_high_var])\n",
    "values = np.hstack([values_low_var, values_high_var])\n",
    "\n",
    "# Compute pairwise distances\n",
    "distances = squareform(pdist(points))\n",
    "\n",
    "# Compute pairwise semi-variance (for values)\n",
    "semi_variances = np.zeros_like(distances)\n",
    "for i in range(len(values)):\n",
    "    for j in range(i + 1, len(values)):\n",
    "        diff = values[i] - values[j]\n",
    "        semi_variances[i, j] = (diff ** 2) / 2\n",
    "\n",
    "# Bin distances into lags\n",
    "lag_bins = np.linspace(0, np.max(distances), 20)  # 20 bins\n",
    "bin_indices = np.digitize(distances, lag_bins)\n",
    "average_semi_variances = [\n",
    "    semi_variances[bin_indices == i].mean() if np.any(bin_indices == i) else np.nan\n",
    "    for i in range(len(lag_bins))\n",
    "]\n",
    "\n",
    "# Plot the semi-variogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(lag_bins[:-1], average_semi_variances[:-1], marker='o', linestyle='-')\n",
    "plt.xlabel(\"Lag (Distance)\", fontsize=12)\n",
    "plt.ylabel(\"Semi-Variance\", fontsize=12)\n",
    "plt.title(\"Semi-Variogram\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize the point cloud and highlight high-variance region\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(points_low_var[:, 0], points_low_var[:, 1], c='blue', label='Low Variance Region')\n",
    "plt.scatter(points_high_var[:, 0], points_high_var[:, 1], c='red', label='High Variance Region')\n",
    "plt.xlabel(\"X Coordinate\", fontsize=12)\n",
    "plt.ylabel(\"Y Coordinate\", fontsize=12)\n",
    "plt.title(\"Spatial Data with High-Variance Region\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Generate synthetic point cloud data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Region 1: Gaussian with mean 0, variance 1\n",
    "N1 = 200\n",
    "points_low_var = np.random.rand(N1, 2) * 50  # Points in lower-left region\n",
    "values_low_var = np.random.normal(0, 1, N1)  # Scalar values\n",
    "\n",
    "# Region 2: Gaussian with mean 0, variance 5 (upper-right region)\n",
    "N2 = 50\n",
    "points_high_var = np.random.rand(N2, 2) * 20 + 50  # Points in upper-right region\n",
    "values_high_var = np.random.normal(0, np.sqrt(5), N2)  # Scalar values\n",
    "\n",
    "# Combine both regions\n",
    "points = np.vstack([points_low_var, points_high_var])\n",
    "values = np.hstack([values_low_var, values_high_var])\n",
    "\n",
    "# Compute pairwise distances\n",
    "distances = squareform(pdist(points))\n",
    "\n",
    "# Compute pairwise semi-variance (for values)\n",
    "semi_variances = np.zeros_like(distances)\n",
    "for i in range(len(values)):\n",
    "    for j in range(i + 1, len(values)):\n",
    "        diff = values[i] - values[j]\n",
    "        semi_variances[i, j] = (diff ** 2) / 2\n",
    "\n",
    "# Scale the semi-variance to the range [0, 1]\n",
    "scaled_semi_variances = (semi_variances - np.min(semi_variances)) / (\n",
    "    np.max(semi_variances) - np.min(semi_variances)\n",
    ")\n",
    "\n",
    "# Define a threshold for scaled uncertainty\n",
    "scaled_uncertainty_threshold = 0.7  # Example threshold in the range [0, 1]\n",
    "\n",
    "# Identify pairs of points with scaled semi-variance above the threshold\n",
    "high_scaled_variance_indices = np.argwhere(scaled_semi_variances > scaled_uncertainty_threshold)\n",
    "unique_high_scaled_var_points = set(high_scaled_variance_indices.flatten())\n",
    "\n",
    "# Extract the points corresponding to high scaled semi-variance\n",
    "high_scaled_variance_points = points[list(unique_high_scaled_var_points)]\n",
    "\n",
    "# Plot only the spatial locations above the scaled threshold\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(points[:, 0], points[:, 1], c='lightgrey', alpha=0.6, label='All Points (Below Threshold)')\n",
    "plt.scatter(\n",
    "    high_scaled_variance_points[:, 0],\n",
    "    high_scaled_variance_points[:, 1],\n",
    "    c='red',\n",
    "    alpha=0.8,\n",
    "    label='High Scaled Variance Points',\n",
    ")\n",
    "plt.xlabel(\"X Coordinate\", fontsize=12)\n",
    "plt.ylabel(\"Y Coordinate\", fontsize=12)\n",
    "plt.title(f\"Spatial Locations Above Scaled Uncertainty Threshold ({scaled_uncertainty_threshold})\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf3Dchange",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
