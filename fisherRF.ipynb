{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rendering import rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisherrf_uncertainty(nerf_model, grid_points, rendering_fn, tn, tf, loss_fn=torch.nn.MSELoss(), device='cuda'):\n",
    "    \"\"\"\n",
    "    Compute FisherRF-based point-wise uncertainty for a grid of points using a trained NeRF model.\n",
    "\n",
    "    Args:\n",
    "        nerf_model (nn.Module): The trained NeRF model instance.\n",
    "        grid_points (torch.Tensor): Tensor of 3D points to query [N^3, 3].\n",
    "        rendering_fn (callable): The rendering function to generate predictions.\n",
    "        tn (float): Near plane distance.\n",
    "        tf (float): Far plane distance.\n",
    "        loss_fn (callable): The loss function used during training (default: MSE).\n",
    "        device (str): Device for computation ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Uncertainty values for each point in the grid [N^3].\n",
    "    \"\"\"\n",
    "    nerf_model.eval()  # Set the model to evaluation mode\n",
    "    grid_points.requires_grad = True  # Enable gradient computation for the grid points\n",
    "\n",
    "    # Generate dummy ray origins and directions\n",
    "    rays_o = torch.zeros_like(grid_points, device=device)  # [N^3, 3], assuming origin at (0, 0, 0)\n",
    "    rays_d = torch.nn.functional.normalize(grid_points - rays_o, dim=-1)  # Normalized ray directions [N^3, 3]\n",
    "\n",
    "    # Use the rendering function to compute predicted colors\n",
    "    predicted_colors = rendering_fn(nerf_model, rays_o, rays_d, tn, tf, device=device)  # [N^3, 3]\n",
    "\n",
    "    # Dummy ground truth for the loss (assume black background)\n",
    "    ground_truth_colors = torch.zeros_like(predicted_colors)\n",
    "\n",
    "    # Compute the loss for each point\n",
    "    # Compute element-wise squared error\n",
    "    losses = ((predicted_colors - ground_truth_colors) ** 2).mean(dim=1)  # Per-point loss [N^3]\n",
    "\n",
    "\n",
    "    uncertainties = []\n",
    "\n",
    "    for i in range(grid_points.size(0)):\n",
    "        # Compute the gradient of the loss w.r.t. the model parameters for each point\n",
    "        loss = losses[i]  # Individual loss for a single point\n",
    "        grads = torch.autograd.grad(\n",
    "            outputs=loss,\n",
    "            inputs=nerf_model.parameters(),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            allow_unused=True\n",
    "        )\n",
    "\n",
    "        # Compute Fisher Information (per point)\n",
    "        fisher_info = 0\n",
    "        for grad in grads:\n",
    "            if grad is not None:\n",
    "                fisher_info += torch.sum(grad ** 2)  # Sum of squared gradients\n",
    "\n",
    "        uncertainties.append(fisher_info.item())  # Store uncertainty for this point\n",
    "\n",
    "    uncertainties = torch.tensor(uncertainties, device=device)  # [N^3]\n",
    "    return uncertainties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_814074/2437194876.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  nerf_model = torch.load(pth_file).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Load the trained NeRF model\n",
    "device = 'cuda'\n",
    "pth_file = 'experiments/suzanne/set100/models/M0.pth'\n",
    "nerf_model = torch.load(pth_file).to(device)\n",
    "\n",
    "# Define the grid points\n",
    "N = 10\n",
    "scale = 1.5\n",
    "x = torch.linspace(-scale, scale, N, device=device)\n",
    "y = torch.linspace(-scale, scale, N, device=device)\n",
    "z = torch.linspace(-scale, scale, N, device=device)\n",
    "x, y, z = torch.meshgrid((x, y, z))\n",
    "grid_points = torch.cat((x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)), dim=1).to(device)  # [N^3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point-wise uncertainties: tensor([3.1778e-02, 3.4055e-02, 6.4941e-02, 3.5116e+00, 3.6575e+00, 8.0486e-01,\n",
      "        3.3760e-01, 1.4387e-01, 4.1386e-02, 2.5016e-02, 1.9511e-02, 4.1798e-02,\n",
      "        1.2863e-01, 5.9680e+00, 4.4475e+00, 6.2833e-01, 2.5494e-01, 1.1609e-01,\n",
      "        3.8760e-02, 2.3718e-02, 1.2359e-02, 1.8760e-02, 4.6280e-01, 2.0988e+00,\n",
      "        5.1935e-01, 4.8900e-01, 2.4326e-01, 9.0937e-02, 4.2602e-02, 2.7437e-02,\n",
      "        8.9870e-03, 2.5878e-02, 1.2493e-01, 1.9307e-01, 5.0776e-01, 5.0966e-01,\n",
      "        2.7463e-01, 9.0608e-02, 5.1866e-02, 3.1535e-02, 1.4036e-02, 1.7021e-01,\n",
      "        1.2156e+00, 2.2033e+00, 5.7687e-01, 4.3933e-01, 2.1984e-01, 9.7455e-02,\n",
      "        4.7324e-02, 2.1598e-02, 1.7828e-02, 2.5008e-01, 1.6689e+00, 5.2316e+00,\n",
      "        2.7615e+00, 5.0480e-01, 2.0728e-01, 8.4930e-02, 4.1830e-02, 2.0848e-02,\n",
      "        1.8024e-02, 2.7630e-02, 6.4993e-01, 1.1846e+00, 3.1882e+00, 5.5400e-01,\n",
      "        2.3471e-01, 8.8975e-02, 5.8275e-02, 3.2752e-02, 3.3800e-02, 1.0455e-02,\n",
      "        2.8756e-02, 1.9419e-01, 2.9395e+00, 5.8549e-01, 3.5030e-01, 1.3210e-01,\n",
      "        7.3887e-02, 4.0209e-02, 3.1638e-02, 5.0242e-02, 6.1371e-02, 2.2784e+00,\n",
      "        3.3014e+00, 6.0922e-01, 2.1427e+00, 1.5448e+00, 1.2789e-01, 4.7494e-02,\n",
      "        8.7838e-02, 3.9326e-02, 3.0406e+00, 5.0954e+00, 4.2328e+00, 4.5720e-01,\n",
      "        2.7281e-01, 2.5865e-01, 1.3708e-01, 7.0813e-02, 1.5891e-02, 3.8707e-02,\n",
      "        6.2662e-02, 5.6002e+00, 2.0994e+00, 8.0166e-01, 3.0550e-01, 1.2613e-01,\n",
      "        3.8311e-02, 1.2915e-02, 6.9946e-03, 3.1778e-02, 4.3303e-02, 7.6444e-01,\n",
      "        2.2801e+00, 7.5005e-01, 2.3487e-01, 7.2645e-02, 2.5016e-02, 1.5193e-02,\n",
      "        4.5836e-03, 2.0066e-02, 5.9807e-02, 4.5952e-01, 3.1203e+00, 5.0697e-01,\n",
      "        1.1413e-01, 3.9552e-02, 2.7885e-02, 1.2691e-02, 1.2564e-03, 8.0750e-03,\n",
      "        2.4861e-02, 2.4984e+00, 2.5186e-01, 4.2800e-01, 1.5987e-01, 5.1312e-02,\n",
      "        3.5082e-02, 1.3144e-02, 5.6182e-03, 2.1152e-02, 3.0197e-01, 8.7744e-01,\n",
      "        5.8397e-01, 3.9252e-01, 9.4664e-02, 4.8483e-02, 2.1091e-02, 5.0424e-03,\n",
      "        5.1010e-03, 3.6263e-02, 4.4094e-01, 1.3565e+00, 3.5409e+00, 4.6997e-01,\n",
      "        1.6358e-01, 5.7110e-02, 2.6364e-02, 5.5736e-03, 2.1402e-03, 1.2660e-02,\n",
      "        9.0617e-02, 2.2770e-01, 4.0326e+00, 5.2759e-01, 1.9708e-01, 6.1878e-02,\n",
      "        2.9282e-02, 1.3980e-02, 5.0320e-03, 1.7367e-02, 5.8624e-02, 3.8167e-01,\n",
      "        3.0818e+00, 5.3999e-01, 2.2750e-01, 1.3857e-01, 4.0015e-02, 2.5808e-02,\n",
      "        2.5568e-02, 8.7837e-02, 2.8578e-02, 3.5217e+00, 4.1802e+00, 4.4828e-01,\n",
      "        2.3629e-01, 1.5222e-01, 7.0812e-02, 4.9941e-02, 2.8167e-02, 9.8791e-02,\n",
      "        3.3854e+00, 3.1730e+00, 2.5220e+00, 7.3975e-01, 2.0361e-01, 1.0006e-01,\n",
      "        8.1907e-02, 6.7542e-02, 8.7841e-03, 1.6211e-02, 6.7117e-02, 1.5160e+01,\n",
      "        1.6569e+00, 1.0671e+00, 2.8427e-01, 8.1050e-02, 1.1930e+00, 1.1537e-02,\n",
      "        4.2864e-03, 1.1424e-02, 4.1074e-02, 2.1093e-01, 1.9632e+00, 6.9748e-01,\n",
      "        1.9301e-01, 5.8702e-02, 1.3803e-02, 1.0897e-02, 1.2407e-03, 3.9047e-03,\n",
      "        3.1778e-02, 4.9498e-02, 2.2494e+00, 5.2547e-01, 1.2754e-01, 2.5016e-02,\n",
      "        1.7577e-02, 6.1752e-03, 1.3309e-03, 1.5548e-03, 1.7022e-02, 1.6536e-01,\n",
      "        1.9589e+00, 3.6863e-01, 6.3952e-02, 2.7894e-02, 9.0802e-03, 2.1043e-03,\n",
      "        2.6165e-03, 3.2409e-03, 1.5549e-02, 4.2756e-02, 2.9265e-01, 3.6644e-01,\n",
      "        7.6428e-02, 2.3974e-02, 4.4147e-03, 8.8841e-04, 1.1023e-03, 3.7662e-03,\n",
      "        2.7257e-02, 6.7219e-01, 4.0941e+00, 3.6040e-01, 8.8772e-02, 2.5641e-02,\n",
      "        4.1720e-03, 4.1729e-02, 1.2808e-03, 2.2527e-03, 2.3280e-02, 3.2488e-02,\n",
      "        3.5737e+00, 4.8549e-01, 1.2552e-01, 3.9966e-02, 2.0877e-02, 4.0354e-03,\n",
      "        1.7935e-03, 1.2175e-02, 8.7838e-02, 1.2933e+00, 3.3061e+00, 3.5532e-01,\n",
      "        2.4729e-01, 7.0813e-02, 3.7731e-02, 1.1598e-02, 3.7655e-03, 2.5570e-02,\n",
      "        8.1350e-02, 4.4620e+00, 2.5506e+00, 6.2834e-01, 3.6718e-01, 1.2693e-01,\n",
      "        5.9233e-02, 4.0865e-02, 9.7736e-03, 5.2243e-02, 7.1578e+00, 2.8503e+00,\n",
      "        2.8340e+00, 9.7151e-01, 4.3811e-01, 6.9180e-02, 7.0295e-02, 4.4895e-02,\n",
      "        4.3434e-03, 1.5591e-02, 1.0645e-01, 2.2105e+00, 1.3717e+00, 8.5216e-01,\n",
      "        1.3272e-01, 5.5352e-02, 2.1141e-01, 8.5653e-03, 2.3921e-03, 4.8224e-03,\n",
      "        3.0592e-02, 3.8322e-01, 1.6461e+00, 7.7021e-01, 7.5103e-02, 8.2017e-02,\n",
      "        9.2349e-03, 7.3886e-03, 3.5001e-04, 1.3975e-03, 8.3772e-03, 7.1571e-02,\n",
      "        1.9518e+00, 5.7830e-01, 7.5154e-02, 1.6356e-02, 2.5603e-01, 4.4930e-03,\n",
      "        2.1525e-04, 3.0315e-04, 1.5379e-03, 3.1778e-02, 3.5116e+00, 3.3760e-01,\n",
      "        2.5016e-02, 5.1456e-03, 2.5486e-03, 9.6906e-04, 4.1265e-04, 1.5952e-03,\n",
      "        1.3178e-03, 8.9870e-03, 1.9307e-01, 2.7463e-01, 3.1535e-02, 2.3438e-03,\n",
      "        5.4874e-04, 4.1836e-04, 3.9162e-04, 1.0637e-03, 1.7258e-03, 1.8024e-02,\n",
      "        1.1846e+00, 2.3471e-01, 3.2752e-02, 3.2694e-03, 5.8537e-04, 3.0142e-04,\n",
      "        3.7883e-04, 5.0239e-04, 3.2684e-03, 8.7837e-02, 5.0954e+00, 2.7281e-01,\n",
      "        7.0812e-02, 1.7288e-02, 2.9475e-03, 1.0740e-03, 6.1013e-04, 1.4702e-03,\n",
      "        1.3504e-02, 3.8347e+00, 2.5769e+00, 1.2243e+00, 3.8932e-02, 5.3891e-02,\n",
      "        2.3606e-02, 2.9165e-03, 1.6398e-03, 8.3849e-03, 2.8112e-02, 3.4177e+00,\n",
      "        2.5984e+00, 9.8694e-01, 1.9042e+00, 1.6675e-02, 1.5630e-02, 8.8430e-03,\n",
      "        3.0861e-03, 1.6333e-02, 5.8527e+00, 2.5405e+00, 1.9825e+00, 5.6968e-01,\n",
      "        1.0541e+00, 7.0088e-02, 2.2122e-02, 1.4647e-02, 8.0391e-03, 1.8010e-02,\n",
      "        1.4045e+00, 2.1381e+00, 2.2558e+00, 5.0612e-01, 1.2040e-01, 4.4052e-02,\n",
      "        1.2339e-02, 6.3348e-03, 1.1333e-03, 7.4868e-03, 2.2680e-02, 3.4144e+00,\n",
      "        1.8566e+00, 5.2890e-01, 7.8841e-02, 3.2005e-02, 4.8237e-03, 4.2247e-03,\n",
      "        2.1878e-04, 9.8305e-04, 5.4312e-03, 7.8172e-02, 1.8819e+00, 3.3544e-01,\n",
      "        1.6719e+00, 3.1786e-03, 3.0742e-03, 1.6406e-03, 9.8311e-05, 8.1629e-05,\n",
      "        3.2260e-04, 4.3434e-03, 2.2105e+00, 1.3272e-01, 8.5653e-03, 5.6262e-03,\n",
      "        5.0017e-04, 3.7973e-04, 4.7435e-05, 1.3219e-04, 1.3063e-04, 2.1525e-04,\n",
      "        3.1778e-02, 2.5016e-02, 9.6907e-04, 2.2438e-04, 1.9868e-04, 1.5292e-04,\n",
      "        4.5666e-05, 6.2561e-05, 1.1020e-04, 3.7883e-04, 8.7837e-02, 7.0812e-02,\n",
      "        1.0740e-03, 1.8253e-04, 2.5776e-04, 1.7714e-04, 1.1990e-04, 2.9334e-04,\n",
      "        3.3497e-04, 3.0861e-03, 2.5405e+00, 1.0541e+00, 1.4647e-02, 1.4983e-03,\n",
      "        7.7193e-04, 4.3472e-04, 4.3788e-04, 1.0733e-03, 4.5153e-03, 2.1193e+00,\n",
      "        2.8536e+00, 1.9296e-01, 2.4958e-02, 1.6244e-02, 2.9117e-03, 2.1522e-03,\n",
      "        1.2336e-03, 4.2302e-03, 2.9136e-02, 2.5931e+00, 3.4925e+00, 3.5788e-01,\n",
      "        8.9185e-01, 2.3208e+00, 2.4361e-02, 1.7280e-02, 5.4257e-03, 7.8922e-03,\n",
      "        3.3484e+00, 2.4010e+00, 3.8696e+00, 3.0444e-01, 1.6548e+00, 3.5087e-02,\n",
      "        5.6785e-02, 2.8002e-02, 6.8572e-03, 2.1516e-02, 3.0776e+00, 2.0352e+00,\n",
      "        3.2007e+00, 3.3464e-01, 1.7328e-01, 4.1690e+00, 3.1474e-02, 4.4071e-03,\n",
      "        1.0104e-03, 6.6007e-03, 7.5386e-02, 2.1245e+00, 1.2623e+00, 4.3449e-01,\n",
      "        3.7428e-02, 7.9605e-03, 5.1643e-03, 4.0968e-03, 3.3378e-04, 5.7948e-04,\n",
      "        6.4992e-03, 8.7527e-01, 1.4788e+00, 5.1677e+00, 2.4340e-02, 2.8023e-03,\n",
      "        1.9767e-03, 1.3122e-03, 1.4350e-04, 1.9540e-04, 3.7404e-04, 9.0197e-03,\n",
      "        1.9950e+00, 1.8836e-01, 5.0111e-03, 3.3113e-03, 5.0714e-04, 3.4322e-04,\n",
      "        9.7176e-05, 1.1784e-04, 2.1742e-04, 4.1500e-04, 8.4484e-02, 2.1215e-03,\n",
      "        1.4440e-03, 2.9061e-04, 2.2510e-04, 1.9941e-04, 4.3110e-05, 6.7687e-05,\n",
      "        1.6713e-04, 2.9119e-04, 6.4470e-02, 3.0864e-02, 7.1117e-04, 2.0542e-04,\n",
      "        1.0838e-04, 1.1477e-04, 1.3349e-04, 2.2907e-04, 2.3869e-04, 5.5696e-03,\n",
      "        3.5448e+00, 1.6385e+00, 3.6393e-02, 2.4105e-03, 7.9823e-04, 2.8153e-04,\n",
      "        3.5743e-04, 2.0360e-03, 5.1725e-03, 4.0809e-01, 3.0260e+00, 2.6011e-01,\n",
      "        4.1389e-02, 3.5910e-02, 5.1396e-03, 3.7887e-03, 1.1040e-03, 3.9469e-03,\n",
      "        4.8739e-02, 4.4533e+00, 2.8016e+00, 8.4353e-01, 6.9399e-01, 1.2816e-02,\n",
      "        3.7445e-02, 6.3961e-03, 3.2323e-03, 1.3720e-02, 6.1587e-01, 2.3664e+00,\n",
      "        1.4253e+00, 5.0813e-01, 1.1488e+00, 4.3147e-02, 2.4343e-02, 2.3216e-02,\n",
      "        9.0197e-03, 8.4072e-02, 8.9252e-01, 1.9950e+00, 2.5234e+00, 5.0285e-01,\n",
      "        1.8836e-01, 2.8918e-02, 7.0770e-03, 5.0111e-03, 2.2091e-03, 1.4616e-02,\n",
      "        9.0658e-02, 2.6190e+00, 1.7409e+00, 4.8976e-01, 4.8351e-02, 2.2489e-02,\n",
      "        6.1504e-03, 7.7174e-03, 6.1885e-04, 2.5603e-03, 2.1804e-02, 4.5406e-01,\n",
      "        1.8748e+00, 1.7997e+00, 2.7305e-01, 8.3145e-03, 5.2637e-03, 6.4980e-03,\n",
      "        4.1500e-04, 4.0983e-04, 7.6064e-03, 8.4484e-02, 5.6495e+00, 1.1990e+00,\n",
      "        2.1215e-03, 4.7259e-03, 9.5561e-03, 1.4440e-03, 1.6517e-04, 3.4420e-04,\n",
      "        4.0029e-03, 2.9663e-02, 4.2930e+00, 4.4330e-01, 3.0777e-03, 2.4699e-03,\n",
      "        1.1428e-03, 5.6817e-04, 1.8574e-04, 3.2144e-04, 3.7119e-03, 2.0691e-02,\n",
      "        4.8877e+00, 1.1844e-01, 8.3725e-03, 2.1370e-03, 6.8841e-04, 2.5645e-04,\n",
      "        2.9119e-04, 5.0230e-04, 3.0018e-03, 6.4470e-02, 6.8812e+00, 2.7620e+00,\n",
      "        3.0864e-02, 3.5959e-03, 3.6380e-03, 7.1118e-04, 5.0850e-04, 1.3616e-03,\n",
      "        1.4825e-02, 1.0270e-01, 1.8542e+00, 4.2872e-01, 9.2381e-02, 4.6484e-02,\n",
      "        3.9417e-03, 3.4982e-03, 1.3202e-03, 6.5547e-03, 2.9318e-02, 1.2554e+00,\n",
      "        4.4359e+00, 4.8995e+00, 2.0324e+00, 2.8184e-02, 3.2331e-02, 1.2316e-02,\n",
      "        5.5696e-03, 2.0898e-02, 7.9196e-02, 3.5448e+00, 4.3998e+00, 1.4756e+00,\n",
      "        1.6385e+00, 4.6103e-02, 2.0789e-02, 3.6393e-02, 2.8306e-02, 9.1513e-02,\n",
      "        1.4842e+00, 2.1460e+00, 1.5529e+00, 1.0556e+00, 1.0317e-01, 3.0397e-02,\n",
      "        5.9487e-02, 8.5400e-03, 1.1286e-02, 5.4328e-02, 2.7131e-01, 3.5341e+00,\n",
      "        1.7694e+00, 1.6070e+00, 1.6405e+00, 1.1289e-02, 3.9675e-03, 7.5756e-03,\n",
      "        3.6983e-03, 1.4178e-02, 8.4484e-02, 3.0064e-01, 3.2648e+00, 5.5807e+00,\n",
      "        8.0549e-02, 2.1215e-03, 2.4298e-02, 2.2205e-03, 1.7431e-03, 1.1581e-02,\n",
      "        5.5848e-02, 2.4759e-01, 4.6620e+00, 1.9220e-01, 1.9829e-02, 3.3149e-03,\n",
      "        2.0326e-03, 3.0581e-03, 9.2075e-04, 6.5550e-03, 2.5814e-02, 1.3980e-01,\n",
      "        2.9088e+00, 9.2580e-02, 1.9596e-02, 3.2349e-03, 1.7003e-03, 1.3375e-03,\n",
      "        1.5337e-03, 7.3985e-03, 2.3001e-02, 1.6595e-01, 2.6078e+00, 1.5230e+00,\n",
      "        5.3196e-02, 1.3059e-02, 3.8091e-03, 1.7110e-03, 1.8752e-03, 8.9372e-03,\n",
      "        3.8951e-02, 1.7010e-01, 3.8541e+00, 2.1000e-01, 7.8475e-02, 4.8496e-03,\n",
      "        4.8136e-03, 3.0175e-03, 1.4722e-03, 1.5755e-02, 6.4470e-02, 2.3088e-01,\n",
      "        8.1208e+00, 4.2683e-01, 1.1910e-01, 3.0865e-02, 4.7892e-03, 4.9421e-03,\n",
      "        5.9615e-03, 3.5106e-02, 4.7325e-02, 4.6772e-01, 1.0453e+00, 4.6935e-01,\n",
      "        4.3301e-01, 2.9350e-02, 1.1117e+00, 1.3324e-02, 1.0949e-02, 3.4294e-02,\n",
      "        1.2345e-01, 6.4729e+00, 9.6295e-01, 1.1154e+00, 1.2123e+00, 2.0432e-01,\n",
      "        1.4218e-02, 3.4334e-02, 7.2668e-02, 1.6182e-01, 4.4082e-01, 4.4950e+00,\n",
      "        1.7175e+00, 1.7928e+00, 1.1723e-01, 5.2526e-02, 1.0064e-02, 7.2814e-03,\n",
      "        2.5582e-02, 8.4484e-02, 1.7039e-01, 1.9783e-01, 3.2845e+00, 3.1961e+00,\n",
      "        2.2102e-01, 2.1476e-02, 2.1215e-03, 2.3611e-02, 2.5296e-02, 6.4777e-02,\n",
      "        1.7132e-01, 3.3116e-01, 1.0573e+01, 2.1948e-01, 9.7411e-02, 1.0292e-02,\n",
      "        4.1829e-03, 4.7027e-03, 8.8703e-03, 3.6938e-02, 1.4374e-01, 3.1768e-01,\n",
      "        3.7206e-01, 6.0471e+00, 4.3093e-02, 1.1982e-02, 3.1481e-03, 2.2110e-03,\n",
      "        8.1665e-03, 2.5727e-02, 4.3652e-02, 2.1524e-01, 3.9638e+00, 2.1320e-01,\n",
      "        6.4456e-02, 1.6543e-02, 4.3084e-03, 3.0717e-03, 1.8872e-02, 2.1983e-02,\n",
      "        6.6334e-02, 2.4473e+00, 3.2095e+00, 2.0844e-01, 1.0017e-01, 2.0580e-02,\n",
      "        1.8333e-02, 3.5577e-03, 1.7111e-02, 1.9707e-02, 1.0707e-01, 3.1904e-01,\n",
      "        3.3409e+00, 3.3087e+00, 1.9165e-01, 4.3223e-02, 6.0183e-03, 4.9132e-03,\n",
      "        1.2067e-02, 1.7775e-02, 8.4877e-02, 1.9243e-01, 1.3247e+01, 3.0850e-01,\n",
      "        2.2464e-01, 6.2327e-02, 1.3344e-02, 3.9993e-03, 2.7924e-02, 6.4470e-02,\n",
      "        1.5203e-01, 2.3470e-01, 6.9996e-01, 5.3884e-01, 2.3308e+00, 6.8038e-02,\n",
      "        3.0864e-02, 1.4071e+00, 3.5573e-02, 4.5133e-02, 1.9996e-01, 1.7622e+00,\n",
      "        1.4978e+00, 1.4329e+00, 1.8343e-01, 9.8230e-02, 2.7147e-02, 1.4190e+00,\n",
      "        8.4484e-02, 1.4707e-01, 3.5515e-01, 5.6495e+00, 3.0073e+00, 2.2378e+00,\n",
      "        1.1990e+00, 1.0144e-01, 1.1985e-02, 2.1215e-03, 5.2192e-02, 1.2400e-01,\n",
      "        2.8526e-01, 6.0594e-01, 6.1907e+00, 2.7276e+00, 4.6049e-02, 3.0441e-02,\n",
      "        6.0271e-03, 3.3235e-03, 5.5658e-02, 1.8733e-01, 2.8453e-01, 2.0922e-01,\n",
      "        2.7385e+00, 4.1889e-01, 5.8565e-02, 2.7329e-02, 8.2998e-03, 3.7667e-03,\n",
      "        2.9663e-02, 7.1549e-02, 2.7006e-01, 4.2930e+00, 3.3661e+00, 3.5494e+00,\n",
      "        4.4331e-01, 3.3033e-02, 9.7882e-03, 3.0777e-03, 2.6520e-02, 4.6431e-02,\n",
      "        1.9366e-01, 6.6899e+00, 5.1397e+00, 8.4079e-01, 1.1930e-01, 2.7251e-02,\n",
      "        1.3023e-02, 5.0744e-03, 1.9737e-02, 5.1763e-02, 1.6132e-01, 3.7360e+00,\n",
      "        3.5659e+00, 2.7476e-01, 1.8472e+00, 6.6189e-02, 1.6181e-01, 1.3927e-02,\n",
      "        2.0691e-02, 5.8923e-02, 2.4610e-01, 4.8877e+00, 3.0964e+00, 2.8725e+00,\n",
      "        1.1844e-01, 5.6990e-02, 1.4818e-02, 8.3725e-03, 3.4348e-02, 9.3418e-02,\n",
      "        1.8352e-01, 2.7675e-01, 6.3086e+00, 3.2537e+00, 1.9319e-01, 1.0617e-01,\n",
      "        3.0245e-02, 5.3788e-03, 2.5580e-02, 7.6743e-02, 2.7282e-01, 1.1277e+01,\n",
      "        1.2020e+01, 3.3425e-01, 2.0607e-01, 3.6247e-01, 3.4734e-02, 2.0541e-02,\n",
      "        6.4470e-02, 1.0926e-01, 2.4061e-01, 6.8812e+00, 8.3800e-01, 6.0078e-01,\n",
      "        2.7620e+00, 1.0630e-01, 5.1866e-02, 3.0865e-02], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def rendering_fn(model, rays_o, rays_d, tn, tf, device='cuda'):\n",
    "    t = torch.linspace(tn, tf, 100).to(device)\n",
    "    delta = torch.cat((t[1:] - t[:-1], torch.tensor([1e10], device=device)))\n",
    "    x = rays_o.unsqueeze(1) + t.unsqueeze(0).unsqueeze(-1) * rays_d.unsqueeze(1)  # [nb_rays, nb_bins, 3]\n",
    "    colors, density = model.intersect(x.reshape(-1, 3), rays_d.expand(x.shape[1], x.shape[0], 3).transpose(0, 1).reshape(-1, 3))\n",
    "    colors = colors.reshape((x.shape[0], 100, 3))  # [nb_rays, nb_bins, 3]\n",
    "    density = density.reshape((x.shape[0], 100))\n",
    "    alpha = 1 - torch.exp(-density * delta.unsqueeze(0))  # [nb_rays, nb_bins]\n",
    "    weights = torch.cumprod(1 - alpha + 1e-10, dim=1) * alpha  # [nb_rays, nb_bins]\n",
    "    return (weights.unsqueeze(-1) * colors).sum(1)  # Rendered color\n",
    "# Compute uncertainties\n",
    "uncertainties = compute_fisherrf_uncertainty(\n",
    "    nerf_model,\n",
    "    grid_points,\n",
    "    rendering_fn,\n",
    "    tn=2.0,  # Near plane\n",
    "    tf=6.0,  # Far plane\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Point-wise uncertainties:\", uncertainties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher Information Matrix:\n",
      " tensor([[   14.2811,   -57.1937,   141.3810,   -18.5445,   119.4245,   -39.4169,\n",
      "           -75.3514,    -8.7922,  -108.5234,    76.1082],\n",
      "        [  -57.1937,   229.0522,  -566.2097,    74.2678,  -478.2774,   157.8590,\n",
      "           301.7711,    35.2113,   434.6202,  -304.8021],\n",
      "        [  141.3810,  -566.2097,  1399.6522,  -183.5876,  1182.2865,  -390.2223,\n",
      "          -745.9684,   -87.0413, -1074.3673,   753.4611],\n",
      "        [  -18.5445,    74.2678,  -183.5876,    24.0806,  -155.0765,    51.1841,\n",
      "            97.8461,    11.4169,   140.9211,   -98.8289],\n",
      "        [  119.4245,  -478.2774,  1182.2865,  -155.0765,   998.6776,  -329.6208,\n",
      "          -630.1196,   -73.5238,  -907.5182,   636.4487],\n",
      "        [  -39.4169,   157.8590,  -390.2223,    51.1841,  -329.6208,   108.7938,\n",
      "           207.9756,    24.2671,   299.5331,  -210.0645],\n",
      "        [  -75.3514,   301.7711,  -745.9684,    97.8461,  -630.1196,   207.9756,\n",
      "           397.5765,    46.3901,   572.6022,  -401.5698],\n",
      "        [   -8.7922,    35.2113,   -87.0413,    11.4169,   -73.5238,    24.2671,\n",
      "            46.3901,     5.4129,    66.8125,   -46.8561],\n",
      "        [ -108.5234,   434.6202, -1074.3673,   140.9211,  -907.5182,   299.5331,\n",
      "           572.6022,    66.8125,   824.6799,  -578.3536],\n",
      "        [   76.1082,  -304.8021,   753.4611,   -98.8289,   636.4487,  -210.0645,\n",
      "          -401.5698,   -46.8561,  -578.3536,   405.6033]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a dummy neural network\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_weights):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.randn(num_weights))\n",
    "    \n",
    "    def forward(self, x, d):\n",
    "        # Dummy density and color predictions based on weights\n",
    "        density = torch.dot(self.weights[:len(x)], x)\n",
    "        color = torch.dot(self.weights[len(x):], d)\n",
    "        return density, color\n",
    "\n",
    "# Initialize the neural network\n",
    "num_weights = 10  # Example: 5 for density, 5 for color\n",
    "net = NeuralNetwork(num_weights)\n",
    "\n",
    "# Example inputs\n",
    "x = torch.randn(5)  # 3D spatial point with additional features\n",
    "d = torch.randn(5)  # View direction\n",
    "observed_y = torch.tensor(1.0)  # Observed pixel value\n",
    "noise_variance = 0.1\n",
    "\n",
    "# Forward pass\n",
    "density, color = net(x, d)\n",
    "predicted_y = density + color\n",
    "\n",
    "# Compute the log-likelihood\n",
    "log_likelihood = -0.5 / noise_variance * (observed_y - predicted_y) ** 2\n",
    "\n",
    "# Compute the gradient of the log-likelihood with respect to weights\n",
    "grad_log_likelihood = torch.autograd.grad(log_likelihood, net.weights, retain_graph=True)[0]\n",
    "\n",
    "# Fisher Information Matrix (outer product of gradients)\n",
    "fim = torch.outer(grad_log_likelihood, grad_log_likelihood)\n",
    "\n",
    "# Print the Fisher Information Matrix\n",
    "print(\"Fisher Information Matrix:\\n\", fim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty at the spatial location: 1.163894772529602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_814074/937704225.py:16: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  uncertainty = torch.sqrt(jacobian @ fim_inv @ jacobian.T)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example: Fisher Information Matrix and a neural network output\n",
    "def compute_uncertainty(fim_inv, jacobian):\n",
    "    \"\"\"\n",
    "    Compute uncertainty at a spatial location given FIM inverse and the Jacobian.\n",
    "    \n",
    "    Args:\n",
    "        fim_inv (torch.Tensor): Inverse of the Fisher Information Matrix (dim: num_weights x num_weights).\n",
    "        jacobian (torch.Tensor): Jacobian of the output with respect to weights (dim: num_weights).\n",
    "    \n",
    "    Returns:\n",
    "        float: Uncertainty at the spatial location.\n",
    "    \"\"\"\n",
    "    # Uncertainty = sqrt(J^T FIM^-1 J)\n",
    "    uncertainty = torch.sqrt(jacobian @ fim_inv @ jacobian.T)\n",
    "    return uncertainty.item()\n",
    "\n",
    "# Example FIM inverse (for simplicity, diagonal here)\n",
    "num_weights = 10\n",
    "fim_inv = torch.diag(torch.ones(num_weights) * 0.1)  # Inverse of Fisher Information Matrix\n",
    "\n",
    "# Example Jacobian (output sensitivity to weights)\n",
    "jacobian = torch.randn(num_weights)  # Random Jacobian for illustration\n",
    "\n",
    "# Compute uncertainty at the location\n",
    "uncertainty = compute_uncertainty(fim_inv, jacobian)\n",
    "print(\"Uncertainty at the spatial location:\", uncertainty)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf3Dchange",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
